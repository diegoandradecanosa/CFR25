{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning de un modelo preentrenado\n",
    "\n",
    "El entrenamiento de un modelo partiendo de una inicialización aleatoria de los parámetros del mismo es una tarea muy costosa computacionalmente. Especialmente para modelos con arquitectura de tipo Transformers, donde los requerimientos computacionales pueden implicar el uso de cientos de GPUs durante semanas, e incluso más. Por ello, es más habitual en entornos modestos partir de una inicialización de los pesos procedente de un modelo preentrenado para una tarea más general que aquella que se quiere acometer. \n",
    "\n",
    "Este proceso, que tiene unos requerimientos computacionales mucho más modestos recibe el nombre de **fine-tuning**. En este caso, los pesos ya están inicializados para resolver una tarea más genérica, pero se expone durante varios ciclos de entrenamiento a un *dataset* más pequeño que el inicial, pero diseñado para que el modelo aprenda una tarea más específica. \n",
    "\n",
    "Este notebook va a demostrar cómo realizar dicho proceso utilizando la librería Transformers. Se va a realizar siguiendo dos vías diferentes.\n",
    "\n",
    "La más sencilla es utilizar la clase Trainer de la librería transformers. \n",
    "\n",
    "Una vía un poco más compleja es utilizando un modelo preentrenado utilizando la librería Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del *dataset*\n",
    "\n",
    "El primer paso para hacer fine-tuning de un modelo es descargar un dataset (o crear el nuestro propio) y prepararlo para el entrenamiento. El notebook anterior (04) nos enseñó varias técnicas para realizar este proceso utilizando la librería transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ulc/es/dac/.cache/huggingface/datasets/parquet/yelp_review_full-66f1f8c8d1a2da02/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adbd7967f704f4da4e943cddd69a004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar el dataset de review de yelp\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "# Imprimir un ejemplo del dataset\n",
    "dataset[\"train\"][100]\n",
    "from transformers import AutoTokenizer\n",
    "# Descargar el tokenizador de BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# Función para tokenizar los ejemplos, se aplica truncation y padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "# Aplicar la función a todo el dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Utilizar un subconjunto del dataset reducido si no queremos entrenar con todo el dataset\n",
    "# Si lo utilizamos todo, se tarda mucho en entrenar\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytx",
   "language": "python",
   "name": "mytx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
