{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LfkL2r2Q1tr"
      },
      "source": [
        "# Comenzando: Explorando los Fundamentos de NeMo\n",
        "\n",
        "NeMo es una herramienta para crear aplicaciones de [IA Conversacional](https://developer.nvidia.com/conversational-ai#started).\n",
        "\n",
        "El kit de herramientas NeMo permite a los investigadores componer fácilmente arquitecturas complejas de redes neuronales para IA conversacional utilizando componentes reutilizables, llamados Módulos Neuronales. Los Módulos Neuronales son bloques conceptuales de redes neuronales que toman entradas tipadas y producen salidas tipadas. Estos módulos generalmente representan capas de datos, codificadores, decodificadores, modelos de lenguaje, funciones de pérdida o métodos para combinar activaciones.\n",
        "\n",
        "El kit incluye colecciones extensibles de módulos preconstruidos y modelos listos para usar en tareas como reconocimiento automático de voz (ASR), procesamiento de lenguaje natural (NLP) y síntesis de texto a voz (TTS). Diseñado para velocidad, NeMo puede aprovechar los Tensor Cores de NVIDIA y escalar el entrenamiento a múltiples GPUs y múltiples nodos.\n",
        "\n",
        "Para más información, visita [la documentación de NeMo](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/#)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zLSy94NEQi-e",
        "outputId": "adf5e555-c935-4310-d0bf-7452f9b22488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "sox is already the newest version (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (1.3)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "--2024-12-24 08:13:15--  https://github.com/state-spaces/mamba/releases/download/v2.2.2/mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/725839295/803cdf7f-7024-45d0-b544-9a0fde764302?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241224T081315Z&X-Amz-Expires=300&X-Amz-Signature=16b496cbc4a2092ee07f1e5103c3a9c5c9303c7356f221a69bd70a64da92b54d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmamba_ssm-2.2.2%2Bcu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-24 08:13:15--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/725839295/803cdf7f-7024-45d0-b544-9a0fde764302?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241224T081315Z&X-Amz-Expires=300&X-Amz-Signature=16b496cbc4a2092ee07f1e5103c3a9c5c9303c7356f221a69bd70a64da92b54d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmamba_ssm-2.2.2%2Bcu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343412612 (328M) [application/octet-stream]\n",
            "Saving to: ‘mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl.1’\n",
            "\n",
            "mamba_ssm-2.2.2+cu1 100%[===================>] 327.50M   239MB/s    in 1.4s    \n",
            "\n",
            "2024-12-24 08:13:17 (239 MB/s) - ‘mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl.1’ saved [343412612/343412612]\n",
            "\n",
            "Processing ./mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (2.5.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (1.11.1.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm==2.2.2+cu118torch2.1cxx11abiFALSE) (2024.12.14)\n",
            "mamba-ssm is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "Collecting git+https://github.com/NVIDIA/NeMo-Run.git\n",
            "  Cloning https://github.com/NVIDIA/NeMo-Run.git to /tmp/pip-req-build-rqrr1xry\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo-Run.git /tmp/pip-req-build-rqrr1xry\n",
            "  Resolved https://github.com/NVIDIA/NeMo-Run.git to commit b4e2258f61b88c53b77996b5f9ed871ee666d85f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: catalogue>=2.0.10 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (2.0.10)\n",
            "Requirement already satisfied: cryptography<43.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (42.0.8)\n",
            "Requirement already satisfied: fabric>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (3.2.2)\n",
            "Requirement already satisfied: fiddle>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (0.3.0)\n",
            "Requirement already satisfied: inquirerpy>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (0.3.4)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (3.1.4)\n",
            "Requirement already satisfied: networkx>=3.3 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (3.4.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (2.3.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (13.9.4)\n",
            "Requirement already satisfied: torchx>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (0.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from nemo_run==0.1.dev76+gb4e2258) (0.15.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0->nemo_run==0.1.dev76+gb4e2258) (1.17.1)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (3.5.0)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (5.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (1.2.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from fiddle>=0.3.0->nemo_run==0.1.dev76+gb4e2258) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from fiddle>=0.3.0->nemo_run==0.1.dev76+gb4e2258) (0.20.3)\n",
            "Requirement already satisfied: libcst in /usr/local/lib/python3.10/dist-packages (from fiddle>=0.3.0->nemo_run==0.1.dev76+gb4e2258) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from fiddle>=0.3.0->nemo_run==0.1.dev76+gb4e2258) (4.12.2)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from inquirerpy>=0.3.4->nemo_run==0.1.dev76+gb4e2258) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from inquirerpy>=0.3.4->nemo_run==0.1.dev76+gb4e2258) (3.0.48)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->nemo_run==0.1.dev76+gb4e2258) (3.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3.0->nemo_run==0.1.dev76+gb4e2258) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3.0->nemo_run==0.1.dev76+gb4e2258) (6.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->nemo_run==0.1.dev76+gb4e2258) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->nemo_run==0.1.dev76+gb4e2258) (2.18.0)\n",
            "Requirement already satisfied: pyre-extensions in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (0.0.32)\n",
            "Requirement already satisfied: docstring-parser>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (0.16)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (8.5.0)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (7.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (2024.10.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (1.26.20)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (0.9.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->nemo_run==0.1.dev76+gb4e2258) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->nemo_run==0.1.dev76+gb4e2258) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0->nemo_run==0.1.dev76+gb4e2258) (2.22)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->nemo_run==0.1.dev76+gb4e2258) (0.1.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (4.2.1)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric>=3.2.2->nemo_run==0.1.dev76+gb4e2258) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->inquirerpy>=0.3.4->nemo_run==0.1.dev76+gb4e2258) (0.2.13)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (3.21.0)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (2024.12.14)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions->torchx>=0.7.0->nemo_run==0.1.dev76+gb4e2258) (1.0.0)\n",
            "Collecting megatron-core\n",
            "  Cloning https://github.com/NVIDIA/Megatron-LM to /tmp/pip-install-j5gtg6p4/megatron-core_9c8705e92b38475ab04365e663900929\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/Megatron-LM /tmp/pip-install-j5gtg6p4/megatron-core_9c8705e92b38475ab04365e663900929\n",
            "  Resolved https://github.com/NVIDIA/Megatron-LM to commit 2da43ef4c1b9e76f03b7567360cf7390e877f1b6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from megatron-core) (0.8.0)\n",
            "Collecting flask-restful (from megatron-core)\n",
            "  Using cached Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from megatron-core) (3.9.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from megatron-core) (8.3.4)\n",
            "Collecting pytest-cov (from megatron-core)\n",
            "  Downloading pytest_cov-6.0.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pytest_mock (from megatron-core)\n",
            "  Using cached pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pytest-random-order (from megatron-core)\n",
            "  Downloading pytest_random_order-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from megatron-core) (0.2.0)\n",
            "Collecting tiktoken (from megatron-core)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from megatron-core) (1.17.0)\n",
            "Collecting zarr (from megatron-core)\n",
            "  Using cached zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from megatron-core) (0.19.1)\n",
            "Collecting tensorstore==0.1.45 (from megatron-core)\n",
            "  Using cached tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-modelopt>=0.19.0 (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core)\n",
            "  Downloading nvidia_modelopt-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorstore==0.1.45->megatron-core) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (1.11.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2.10.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (13.9.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (4.67.1)\n",
            "Collecting pulp (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core)\n",
            "  Downloading PuLP-2.9.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2024.11.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (0.4.5)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2.5.1+cu121)\n",
            "Collecting torchprofile>=0.0.4 (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core)\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (0.20.1+cu121)\n",
            "Collecting aniso8601>=0.82 (from flask-restful->megatron-core)\n",
            "  Using cached aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->megatron-core) (3.1.0)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from flask-restful->megatron-core) (1.17.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->megatron-core) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->megatron-core) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->megatron-core) (1.4.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->megatron-core) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->megatron-core) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->megatron-core) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->megatron-core) (2.2.1)\n",
            "Collecting coverage>=7.5 (from coverage[toml]>=7.5->pytest-cov->megatron-core)\n",
            "  Downloading coverage-7.6.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->megatron-core) (2.32.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (75.6.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb->megatron-core) (4.12.2)\n",
            "Collecting asciitree (from zarr->megatron-core)\n",
            "  Using cached asciitree-0.3.3-py3-none-any.whl\n",
            "Collecting numcodecs>=0.10.0 (from zarr->megatron-core)\n",
            "  Using cached numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting fasteners (from zarr->megatron-core)\n",
            "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->megatron-core) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->megatron-core) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->megatron-core) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->megatron-core) (1.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->megatron-core) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->megatron-core) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->megatron-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->megatron-core) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->megatron-core) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (11.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->megatron-core) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask-restful->megatron-core) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt>=0.19.0->nvidia-modelopt[torch]>=0.19.0; sys_platform != \"darwin\"->megatron-core) (0.1.2)\n",
            "Using cached tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Downloading nvidia_modelopt-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Downloading pytest_cov-6.0.0-py3-none-any.whl (22 kB)\n",
            "Using cached pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading pytest_random_order-1.1.1-py3-none-any.whl (11 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached zarr-2.18.3-py3-none-any.whl (210 kB)\n",
            "Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "Downloading coverage-7.6.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (234 kB)\n",
            "Using cached numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: megatron-core\n",
            "  Building wheel for megatron-core (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for megatron-core: filename=megatron_core-0.10.0rc0-cp310-cp310-linux_x86_64.whl size=1691563 sha256=16d4551cffd75471d00f85796b09f66f17f21977e764ac368093bf9a92c9d6e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g5qogmio/wheels/b9/25/96/2984dd1dd83301f3574afa0135c2b727fab10c920398f86695\n",
            "Successfully built megatron-core\n",
            "Installing collected packages: asciitree, aniso8601, tensorstore, pulp, numcodecs, fasteners, coverage, zarr, tiktoken, pytest-random-order, pytest_mock, pytest-cov, nvidia-modelopt, flask-restful, torchprofile, megatron-core\n",
            "  Attempting uninstall: tensorstore\n",
            "    Found existing installation: tensorstore 0.1.71\n",
            "    Uninstalling tensorstore-0.1.71:\n",
            "      Successfully uninstalled tensorstore-0.1.71\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.6.4 requires tensorstore>=0.1.60, but you have tensorstore 0.1.45 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aniso8601-9.0.1 asciitree-0.3.3 coverage-7.6.9 fasteners-0.19 flask-restful-0.3.10 megatron-core-0.10.0rc0 numcodecs-0.13.1 nvidia-modelopt-0.21.1 pulp-2.9.0 pytest-cov-6.0.0 pytest-random-order-1.1.1 pytest_mock-3.14.0 tensorstore-0.1.45 tiktoken-0.8.0 torchprofile-0.0.4 zarr-2.18.3\n",
            "Collecting nemo_toolkit[all]\n",
            "  Using cached nemo_toolkit-2.0.0-py3-none-any.whl.metadata (47 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.27.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.26.4)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit[all])\n",
            "  Using cached onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.8.2)\n",
            "Collecting ruamel.yaml (from nemo_toolkit[all])\n",
            "  Using cached ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.6.0)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (75.6.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.17.1)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.67.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.17.0)\n",
            "Collecting black~=24.3 (from nemo_toolkit[all])\n",
            "  Using cached black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (8.1.7)\n",
            "Collecting isort<6.0.0,>5.1.0 (from nemo_toolkit[all])\n",
            "  Using cached isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting parameterized (from nemo_toolkit[all])\n",
            "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (8.3.4)\n",
            "Requirement already satisfied: pytest-mock in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.14.0)\n",
            "Collecting pytest-runner (from nemo_toolkit[all])\n",
            "  Using cached pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (8.1.3)\n",
            "Collecting sphinxcontrib-bibtex (from nemo_toolkit[all])\n",
            "  Using cached sphinxcontrib_bibtex-2.6.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.19.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: fiddle in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.3.0)\n",
            "Collecting hydra-core<=1.3.2,>1.3 (from nemo_toolkit[all])\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.3.0)\n",
            "Collecting pytorch-lightning>2.2.1 (from nemo_toolkit[all])\n",
            "  Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from nemo_toolkit[all])\n",
            "  Using cached torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers>=4.44.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.47.1)\n",
            "Collecting webdataset>=0.2.86 (from nemo_toolkit[all])\n",
            "  Using cached webdataset-0.2.100-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting datasets (from nemo_toolkit[all])\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.8.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (7.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.2.2)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit[all])\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.2.0)\n",
            "Collecting braceexpand (from nemo_toolkit[all])\n",
            "  Using cached braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.8.1)\n",
            "Collecting g2p-en (from nemo_toolkit[all])\n",
            "  Using cached g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jiwer (from nemo_toolkit[all])\n",
            "  Using cached jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit[all])\n",
            "  Using cached kaldi_python_io-1.2.2-py3-none-any.whl\n",
            "Collecting kaldiio (from nemo_toolkit[all])\n",
            "  Using cached kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lhotse>=1.26.0 (from nemo_toolkit[all])\n",
            "  Using cached lhotse-1.29.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: librosa>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.10.2.post1)\n",
            "Collecting marshmallow (from nemo_toolkit[all])\n",
            "  Using cached marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (24.2)\n",
            "Collecting pyannote.core (from nemo_toolkit[all])\n",
            "  Using cached pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.metrics (from nemo_toolkit[all])\n",
            "  Using cached pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pydub (from nemo_toolkit[all])\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyloudnorm (from nemo_toolkit[all])\n",
            "  Using cached pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting resampy (from nemo_toolkit[all])\n",
            "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.13.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.12.1)\n",
            "Collecting sox (from nemo_toolkit[all])\n",
            "  Using cached sox-1.5.0-py3-none-any.whl\n",
            "Collecting texterrors (from nemo_toolkit[all])\n",
            "  Using cached texterrors-0.5.1-cp310-cp310-linux_x86_64.whl\n",
            "Collecting accelerated-scan (from nemo_toolkit[all])\n",
            "  Using cached accelerated_scan-0.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting boto3 (from nemo_toolkit[all])\n",
            "  Using cached boto3-1.35.87-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting faiss-cpu (from nemo_toolkit[all])\n",
            "  Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting fasttext (from nemo_toolkit[all])\n",
            "  Using cached fasttext-0.9.3-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: flask-restful in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.3.10)\n",
            "Collecting ftfy (from nemo_toolkit[all])\n",
            "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (5.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.12.1)\n",
            "Collecting ijson (from nemo_toolkit[all])\n",
            "  Using cached ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.42.1)\n",
            "Requirement already satisfied: mamba-ssm==2.2.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.2.2)\n",
            "Collecting markdown2 (from nemo_toolkit[all])\n",
            "  Using cached markdown2-2.5.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.8.0)\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.9.1)\n",
            "Collecting opencc<1.1.7 (from nemo_toolkit[all])\n",
            "  Using cached OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting pangu (from nemo_toolkit[all])\n",
            "  Using cached pangu-4.0.6.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting rapidfuzz (from nemo_toolkit[all])\n",
            "  Using cached rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting rouge-score (from nemo_toolkit[all])\n",
            "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
            "Collecting sacrebleu (from nemo_toolkit[all])\n",
            "  Using cached sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.3.1)\n",
            "Requirement already satisfied: tensorstore<0.1.46 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.45)\n",
            "Collecting tiktoken==0.7.0 (from nemo_toolkit[all])\n",
            "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.18.3)\n",
            "Collecting attrdict (from nemo_toolkit[all])\n",
            "  Using cached attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting janome (from nemo_toolkit[all])\n",
            "  Using cached Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kornia (from nemo_toolkit[all])\n",
            "  Using cached kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pypinyin (from nemo_toolkit[all])\n",
            "  Using cached pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pypinyin-dict (from nemo_toolkit[all])\n",
            "  Using cached pypinyin_dict-0.8.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting progress>=1.5 (from nemo_toolkit[all])\n",
            "  Using cached progress-1.6-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.9.0)\n",
            "Collecting textdistance>=4.1.5 (from nemo_toolkit[all])\n",
            "  Using cached textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting addict (from nemo_toolkit[all])\n",
            "  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting clip (from nemo_toolkit[all])\n",
            "  Using cached clip-0.2.0-py3-none-any.whl\n",
            "Collecting decord (from nemo_toolkit[all])\n",
            "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: diffusers>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.31.0)\n",
            "Collecting einops-exts (from nemo_toolkit[all])\n",
            "  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.36.1)\n",
            "Collecting megatron-energon (from nemo_toolkit[all])\n",
            "  Using cached megatron_energon-4.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting nerfacc>=0.5.3 (from nemo_toolkit[all])\n",
            "  Using cached nerfacc-0.5.3-py3-none-any.whl.metadata (915 bytes)\n",
            "Collecting open-clip-torch==2.24.0 (from nemo_toolkit[all])\n",
            "  Using cached open_clip_torch-2.24.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting PyMCubes (from nemo_toolkit[all])\n",
            "  Using cached PyMCubes-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (868 bytes)\n",
            "Collecting taming-transformers (from nemo_toolkit[all])\n",
            "  Using cached taming_transformers-0.0.1-py3-none-any.whl.metadata (499 bytes)\n",
            "Collecting torchdiffeq (from nemo_toolkit[all])\n",
            "  Using cached torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting torchsde (from nemo_toolkit[all])\n",
            "  Using cached torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting trimesh (from nemo_toolkit[all])\n",
            "  Using cached trimesh-4.5.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pesq (from nemo_toolkit[all])\n",
            "  Using cached pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pystoi (from nemo_toolkit[all])\n",
            "  Using cached pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting nemo-text-processing (from nemo_toolkit[all])\n",
            "  Using cached nemo_text_processing-1.1.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2->nemo_toolkit[all]) (1.11.1.3)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm==2.2.2->nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch==2.24.0->nemo_toolkit[all]) (0.20.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch==2.24.0->nemo_toolkit[all]) (2024.11.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch==2.24.0->nemo_toolkit[all]) (4.25.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch==2.24.0->nemo_toolkit[all]) (1.0.12)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0->nemo_toolkit[all]) (2.32.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black~=24.3->nemo_toolkit[all]) (1.0.0)\n",
            "Collecting pathspec>=0.9.0 (from black~=24.3->nemo_toolkit[all])\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black~=24.3->nemo_toolkit[all]) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black~=24.3->nemo_toolkit[all]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black~=24.3->nemo_toolkit[all]) (4.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (3.16.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (11.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo_toolkit[all]) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo_toolkit[all]) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit[all]) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.26.0->nemo_toolkit[all]) (3.0.1)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse>=1.26.0->nemo_toolkit[all])\n",
            "  Using cached cytoolz-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse>=1.26.0->nemo_toolkit[all])\n",
            "  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\n",
            "Collecting lilcom>=1.1.0 (from lhotse>=1.26.0->nemo_toolkit[all])\n",
            "  Using cached lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (5.1.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.2->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[all]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[all]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[all]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[all]) (3.2.0)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.10/dist-packages (from nerfacc>=0.5.3->nemo_toolkit[all]) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[all]) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo_toolkit[all]) (1.17.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>2.2.1->nemo_toolkit[all])\n",
            "  Using cached lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[all]) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->nemo_toolkit[all]) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->nemo_toolkit[all]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.44.0->nemo_toolkit[all]) (0.21.0)\n",
            "Collecting botocore<1.36.0,>=1.35.87 (from boto3->nemo_toolkit[all])\n",
            "  Using cached botocore-1.35.87-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->nemo_toolkit[all])\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->nemo_toolkit[all])\n",
            "  Using cached s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->nemo_toolkit[all])\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->nemo_toolkit[all])\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->nemo_toolkit[all])\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.24->nemo_toolkit[all])\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (3.11.10)\n",
            "Collecting pybind11>=2.2 (from fasttext->nemo_toolkit[all])\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from fiddle->nemo_toolkit[all]) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from fiddle->nemo_toolkit[all]) (0.20.3)\n",
            "Requirement already satisfied: libcst in /usr/local/lib/python3.10/dist-packages (from fiddle->nemo_toolkit[all]) (1.5.1)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (9.0.1)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->nemo_toolkit[all]) (0.2.13)\n",
            "Collecting distance>=0.1.3 (from g2p-en->nemo_toolkit[all])\n",
            "  Using cached Distance-0.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_toolkit[all]) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_toolkit[all]) (4.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->nemo_toolkit[all]) (4.12.3)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia->nemo_toolkit[all])\n",
            "  Using cached kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting cdifflib (from nemo-text-processing->nemo_toolkit[all])\n",
            "  Using cached cdifflib-1.2.6-cp310-cp310-linux_x86_64.whl\n",
            "Collecting pynini==2.1.6.post1 (from nemo-text-processing->nemo_toolkit[all])\n",
            "  Using cached pynini-2.1.6.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_toolkit[all]) (2024.2)\n",
            "Collecting sortedcontainers>=2.0.4 (from pyannote.core->nemo_toolkit[all])\n",
            "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit[all])\n",
            "  Using cached pyannote.database-5.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics->nemo_toolkit[all])\n",
            "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (1.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit[all])\n",
            "  Using cached ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting portalocker (from sacrebleu->nemo_toolkit[all])\n",
            "  Using cached portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting colorama (from sacrebleu->nemo_toolkit[all])\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->nemo_toolkit[all]) (5.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (1.4.1)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex->nemo_toolkit[all])\n",
            "  Using cached pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex->nemo_toolkit[all])\n",
            "  Using cached pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (3.1.3)\n",
            "Collecting plac (from texterrors->nemo_toolkit[all])\n",
            "  Using cached plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting loguru (from texterrors->nemo_toolkit[all])\n",
            "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[all]) (2.5.0)\n",
            "Collecting Levenshtein (from texterrors->nemo_toolkit[all])\n",
            "  Using cached levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting trampoline>=0.1.2 (from torchsde->nemo_toolkit[all])\n",
            "  Using cached trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (3.1.43)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (2.10.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (1.3.4)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.13.1)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.19)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.87->boto3->nemo_toolkit[all]) (1.26.20)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[all]) (2.22)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->lhotse>=1.26.0->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo_toolkit[all]) (1.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[all]) (4.0.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->nemo_toolkit[all]) (3.0.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (0.15.1)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex->nemo_toolkit[all])\n",
            "  Using cached latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->nemo_toolkit[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->nemo_toolkit[all]) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo_toolkit[all]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo_toolkit[all]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo_toolkit[all]) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit[all]) (3.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->nemo_toolkit[all]) (2.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers>=0.19.3->nemo_toolkit[all]) (3.21.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->nemo_toolkit[all]) (1.7.1)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.16.0-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached aiobotocore-2.15.1-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached aiobotocore-2.15.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached aiobotocore-2.14.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached aiobotocore-2.13.3-py3-none-any.whl.metadata (22 kB)\n",
            "  Using cached aiobotocore-2.13.2-py3-none-any.whl.metadata (22 kB)\n",
            "  Using cached aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached aiobotocore-2.13.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.12.4-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.12.3-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.12.2-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached aiobotocore-2.12.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.11.2-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.11.1-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.11.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Using cached aiobotocore-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached aiobotocore-2.9.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached aiobotocore-2.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached aiobotocore-2.8.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Using cached aiobotocore-2.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2024.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.4.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2023.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.4.2 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2023.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2022.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2022.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2022.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2022.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2022.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2022.7.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.3.4 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.3.4-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2022.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2022.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2022.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.2.0 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.2.0.tar.gz (59 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2022.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aiobotocore~=2.1.0 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.1.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2022.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached s3fs-2021.11.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.0.1 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.0.1.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2021.11.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=1.4.1 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-1.4.2.tar.gz (52 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-2021.10.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.8.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.6.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-2021.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "  Using cached s3fs-0.6.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiobotocore>=1.0.1 (from s3fs->megatron-energon->nemo_toolkit[all])\n",
            "  Using cached aiobotocore-2.5.3-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.5.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached aiobotocore-2.3.3.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.3.2.tar.gz (104 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.3.1.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.3.0.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.1.1.tar.gz (57 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.1.0.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-2.0.0.tar.gz (52 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.4.1.tar.gz (52 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.4.0.tar.gz (51 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.3.3.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.3.2.tar.gz (49 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.3.1.tar.gz (48 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.3.0.tar.gz (48 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.2.2.tar.gz (48 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.2.1.tar.gz (48 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.2.0.tar.gz (47 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached aiobotocore-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.0.7-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.0.6-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.0.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached aiobotocore-1.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Using cached aiobotocore-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Using cached aiobotocore-1.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached aiobotocore-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting s3fs (from megatron-energon->nemo_toolkit[all])\n",
            "  Using cached s3fs-0.5.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Using cached s3fs-0.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached s3fs-0.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached s3fs-0.4.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[all]) (5.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo_toolkit[all]) (0.1.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (1.5.4)\n",
            "Using cached open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Using cached isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Using cached lhotse-1.29.0-py3-none-any.whl (843 kB)\n",
            "Using cached nerfacc-0.5.3-py3-none-any.whl (54 kB)\n",
            "Using cached onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "Using cached OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (778 kB)\n",
            "Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Using cached textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Using cached torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "Using cached webdataset-0.2.100-py3-none-any.whl (74 kB)\n",
            "Using cached accelerated_scan-0.2.0-py3-none-any.whl (11 kB)\n",
            "Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Using cached attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Using cached boto3-1.35.87-py3-none-any.whl (139 kB)\n",
            "Using cached braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "Using cached einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Using cached g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "Using cached ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Using cached kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "Using cached markdown2-2.5.2-py3-none-any.whl (48 kB)\n",
            "Using cached marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "Downloading megatron_energon-4.0.0-py3-none-any.whl (168 kB)\n",
            "Using cached nemo_text_processing-1.1.0-py3-none-any.whl (2.7 MB)\n",
            "Using cached pynini-2.1.6.post1-cp310-cp310-manylinux_2_28_x86_64.whl (154.5 MB)\n",
            "Downloading nemo_toolkit-2.0.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Using cached pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "Using cached pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Using cached pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Using cached PyMCubes-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "Using cached pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "Using cached pypinyin_dict-0.8.0-py2.py3-none-any.whl (9.5 MB)\n",
            "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Using cached pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\n",
            "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "Using cached ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "Using cached sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "Using cached sphinxcontrib_bibtex-2.6.3-py3-none-any.whl (40 kB)\n",
            "Using cached taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
            "Using cached torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Using cached torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "Using cached trimesh-4.5.3-py3-none-any.whl (704 kB)\n",
            "Using cached botocore-1.35.87-py3-none-any.whl (13.3 MB)\n",
            "Using cached cytoolz-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Using cached kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Using cached lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Using cached lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Using cached pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Using cached pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "Using cached pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Using cached ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "Using cached s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Using cached trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Using cached plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Using cached portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
            "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: pesq\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp310-cp310-linux_x86_64.whl size=262952 sha256=d99c710ab408e70f97e98b6998d15e199c50d7a2d47f7b113adc85d413b2c2eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4e/2c/251524370c0fdd659e99639a0fbd0ca5a782c3aafcd456b28d\n",
            "Successfully built pesq\n",
            "Installing collected packages: trampoline, sortedcontainers, pydub, progress, plac, pesq, pangu, opencc, janome, ijson, docopt, distance, clip, braceexpand, addict, xxhash, webdataset, trimesh, textdistance, sox, sacremoses, ruamel.yaml.clib, rapidfuzz, pytest-runner, pypinyin, pynini, pybind11, portalocker, pathspec, parameterized, onnx, marshmallow, markdown2, loguru, lilcom, lightning-utilities, latexcodec, kornia-rs, kaldiio, kaldi-python-io, jmespath, isort, intervaltree, ftfy, fsspec, faiss-cpu, einops-exts, dill, decord, cytoolz, colorama, cdifflib, attrdict, tiktoken, sacrebleu, ruamel.yaml, rouge-score, resampy, pystoi, pypinyin-dict, PyMCubes, pyloudnorm, pybtex, pyannote.core, multiprocess, Levenshtein, jiwer, hydra-core, fasttext, botocore, black, torchsde, torchmetrics, torchdiffeq, texterrors, s3transfer, s3fs, pybtex-docutils, nerfacc, nemo_toolkit, lhotse, kornia, g2p-en, accelerated-scan, sphinxcontrib-bibtex, pyannote.database, megatron-energon, boto3, pytorch-lightning, pyannote.metrics, open-clip-torch, nemo-text-processing, datasets, taming-transformers\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.8.0\n",
            "    Uninstalling tiktoken-0.8.0:\n",
            "      Successfully uninstalled tiktoken-0.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.26.1 PyMCubes-0.1.6 accelerated-scan-0.2.0 addict-2.4.0 attrdict-2.0.1 black-24.10.0 boto3-1.35.87 botocore-1.35.87 braceexpand-0.1.7 cdifflib-1.2.6 clip-0.2.0 colorama-0.4.6 cytoolz-1.0.1 datasets-3.2.0 decord-0.6.0 dill-0.3.8 distance-0.1.3 docopt-0.6.2 einops-exts-0.0.4 faiss-cpu-1.9.0.post1 fasttext-0.9.3 fsspec-2024.9.0 ftfy-6.3.1 g2p-en-2.1.0 hydra-core-1.3.2 ijson-3.3.0 intervaltree-3.1.0 isort-5.13.2 janome-0.5.0 jiwer-3.0.5 jmespath-1.0.1 kaldi-python-io-1.2.2 kaldiio-2.18.0 kornia-0.7.4 kornia-rs-0.1.7 latexcodec-3.0.0 lhotse-1.29.0 lightning-utilities-0.11.9 lilcom-1.8.0 loguru-0.7.3 markdown2-2.5.2 marshmallow-3.23.2 megatron-energon-4.0.0 multiprocess-0.70.16 nemo-text-processing-1.1.0 nemo_toolkit-2.0.0 nerfacc-0.5.3 onnx-1.17.0 open-clip-torch-2.24.0 opencc-1.1.6 pangu-4.0.6.1 parameterized-0.9.0 pathspec-0.12.1 pesq-0.0.4 plac-1.4.3 portalocker-3.0.0 progress-1.6 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pybind11-2.13.6 pybtex-0.24.0 pybtex-docutils-1.0.3 pydub-0.25.1 pyloudnorm-0.1.1 pynini-2.1.6.post1 pypinyin-0.53.0 pypinyin-dict-0.8.0 pystoi-0.4.1 pytest-runner-6.0.1 pytorch-lightning-2.5.0.post0 rapidfuzz-3.11.0 resampy-0.4.3 rouge-score-0.1.2 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.12 s3fs-0.4.2 s3transfer-0.10.4 sacrebleu-2.4.3 sacremoses-0.1.1 sortedcontainers-2.4.0 sox-1.5.0 sphinxcontrib-bibtex-2.6.3 taming-transformers-0.0.1 textdistance-4.6.3 texterrors-0.5.1 tiktoken-0.7.0 torchdiffeq-0.2.5 torchmetrics-1.6.0 torchsde-0.2.6 trampoline-0.1.2 trimesh-4.5.3 webdataset-0.2.100 xxhash-3.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "09dd070d749d4ba19fb463126b9c8bf8",
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘configs’: File exists\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "!pip install Cython packaging\n",
        "!wget https://github.com/state-spaces/mamba/releases/download/v2.2.2/mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "!pip install mamba_ssm-2.2.2+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "!pip install git+https://github.com/NVIDIA/NeMo-Run.git\n",
        "!pip install git+https://github.com/NVIDIA/Megatron-LM#egg=megatron-core\n",
        "!pip install nemo_toolkit['all']\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G2TZkaxcM0e"
      },
      "source": [
        "## Fundamentos de NeMo\n",
        "---------\n",
        "\n",
        "Los modelos de NeMo se basan en el módulo [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) y son compatibles con todo el ecosistema de PyTorch. Esto significa que los usuarios tienen la flexibilidad completa de utilizar las APIs de alto nivel que proporciona PyTorch Lightning (a través de Trainer) o escribir sus propios bucles de entrenamiento y evaluación directamente en PyTorch (simplemente llamando al modelo y sus componentes individuales).\n",
        "\n",
        "Para los desarrolladores de NeMo, un \"Modelo\" es la red neuronal (o redes neuronales) junto con toda la infraestructura que soporta dichas redes, empaquetada en una unidad singular y coherente. Por lo tanto, todos los modelos de NeMo están diseñados para incluir, como mínimo, los siguientes elementos listos para usar (algunos modelos también soportan funcionalidades adicionales):\n",
        "\n",
        " - **Arquitectura de red neuronal**: todos los módulos necesarios para el modelo.\n",
        " - **Conjuntos de datos y cargadores de datos**: todos los componentes que preparan los datos para su uso durante el entrenamiento o evaluación.\n",
        " - **Preprocesamiento y posprocesamiento**: todos los componentes que procesan los conjuntos de datos para que puedan ser consumidos fácilmente por los módulos.\n",
        " - **Optimizador y planificadores de tasa de aprendizaje**: configuraciones predeterminadas que funcionan de manera predeterminada y permiten experimentación adicional con facilidad.\n",
        " - **Cualquier otra infraestructura de soporte**: tokenizadores, configuración de modelos de lenguaje, aumento de datos, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "XxAwtqWBQrNk",
        "outputId": "1fc63618-f16a-442e-adb3-e93b0d4b9ba0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nemo\n",
        "nemo.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H01SHfKQh-gV"
      },
      "source": [
        "## Colecciones de NeMo\n",
        "\n",
        "NeMo se divide en varias colecciones fundamentales según sus dominios: `asr`, `nlp`, `tts`. Cuando ejecutaste la instrucción `import nemo` mencionada anteriormente, ninguna de estas colecciones fue importada. Esto se debe a que es posible que no necesites todas las colecciones al mismo tiempo, por lo que NeMo permite importar parcialmente una o más colecciones según lo necesites.\n",
        "\n",
        "-------\n",
        "Importemos las tres colecciones mencionadas anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J09NNa8fhth7",
        "outputId": "51d49461-91da-4dcb-b0f1-e1a0c732caee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:16:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/optimizer/__init__.py:17: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-24 08:16:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-24 08:16:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/lightning/pytorch/strategies/utils.py:29: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:13 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n",
            "[NeMo W 2024-12-24 08:17:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/models/gpt/gpt_layer_specs.py:50: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
            "      warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
            "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
            "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/models/T5/t5_spec.py:43: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
            "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import nemo.collections.nlp as nemo_nlp\n",
        "import nemo.collections.tts as nemo_tts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSvYoeBrjPza"
      },
      "source": [
        "## Modelos de NeMo en Colecciones\n",
        "\n",
        "NeMo incluye varios modelos para cada una de sus colecciones, relacionados con tareas comunes en la IA conversacional. A continuación, echaremos un vistazo rápido a todos los modelos que NeMo ofrece para las 3 colecciones mencionadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LbbC_92i41f",
        "outputId": "8d4a931c-95ef-429b-ec10-8d3638a06641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ASRModel',\n",
              " 'EncDecCTCModel',\n",
              " 'EncDecClassificationModel',\n",
              " 'EncDecDiarLabelModel',\n",
              " 'EncDecFrameClassificationModel',\n",
              " 'EncDecHybridRNNTCTCBPEModel',\n",
              " 'EncDecHybridRNNTCTCModel',\n",
              " 'EncDecK2RnntSeqModel',\n",
              " 'EncDecK2SeqModel',\n",
              " 'EncDecMultiTaskModel',\n",
              " 'EncDecRNNTBPEModel',\n",
              " 'EncDecRNNTModel',\n",
              " 'EncDecSpeakerLabelModel',\n",
              " 'SLUIntentSlotBPEModel',\n",
              " 'SpeechEncDecSelfSupervisedModel']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "asr_models = [model for model in dir(nemo_asr.models) if model.endswith(\"Model\")]\n",
        "asr_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_ax9Z8j9FC",
        "outputId": "19f6dadc-c45c-409e-edab-a61f7ab086fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['BERTLMModel',\n",
              " 'BertDPRModel',\n",
              " 'BertJointIRModel',\n",
              " 'DuplexDecoderModel',\n",
              " 'DuplexTaggerModel',\n",
              " 'DuplexTextNormalizationModel',\n",
              " 'EntityLinkingModel',\n",
              " 'GLUEModel',\n",
              " 'IntentSlotClassificationModel',\n",
              " 'MTEncDecModel',\n",
              " 'MegatronGPTPromptLearningModel',\n",
              " 'MultiLabelIntentSlotClassificationModel',\n",
              " 'PunctuationCapitalizationLexicalAudioModel',\n",
              " 'PunctuationCapitalizationModel',\n",
              " 'QAModel',\n",
              " 'SpellcheckingAsrCustomizationModel',\n",
              " 'Text2SparqlModel',\n",
              " 'TextClassificationModel',\n",
              " 'ThutmoseTaggerModel',\n",
              " 'TokenClassificationModel',\n",
              " 'TransformerLMModel',\n",
              " 'ZeroShotIntentModel']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp_models = [model for model in dir(nemo_nlp.models) if model.endswith(\"Model\")]\n",
        "nlp_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQdR6RJdkezq",
        "outputId": "37bb6552-406d-4f96-a0d7-3470b0d638b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['AlignerModel',\n",
              " 'AudioCodecModel',\n",
              " 'FastPitchModel',\n",
              " 'GriffinLimModel',\n",
              " 'HifiGanModel',\n",
              " 'MelPsuedoInverseModel',\n",
              " 'MixerTTSModel',\n",
              " 'RadTTSModel',\n",
              " 'SpectrogramEnhancerModel',\n",
              " 'Tacotron2Model',\n",
              " 'TwoStagesModel',\n",
              " 'UnivNetModel',\n",
              " 'VitsModel',\n",
              " 'WaveGlowModel']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tts_models = [model for model in dir(nemo_tts.models) if model.endswith(\"Model\")]\n",
        "tts_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWKxKQnSkj9Z"
      },
      "source": [
        "## El Modelo NeMo\n",
        "\n",
        "Vamos a profundizar en lo que realmente es un modelo NeMo. Hay varias formas de crear estos modelos: podemos usar el constructor y pasar una configuración, instanciar el modelo desde un punto de control preentrenado, o simplemente proporcionar el nombre de un modelo preentrenado e instanciarlo directamente desde la nube.\n",
        "\n",
        "---------\n",
        "Por ahora, trabajemos con un modelo de ASR - [Citrinet](https://arxiv.org/abs/2104.01721)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-XOQaW1kh3v",
        "outputId": "8368fe42-28bb-43fb-8391-9bfd17089887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:17:38 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo to /root/.cache/torch/NeMo/NeMo_2.0.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo\n",
            "[NeMo I 2024-12-24 08:17:40 common:826] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-12-24 08:17:45 mixins:173] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:17:45 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    trim_silence: true\n",
            "    max_duration: 16.7\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:45 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2024-12-24 08:17:45 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 12\n",
            "    pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:17:45 features:305] PADDING: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:17:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/core/connectors/save_restore_connector.py:682: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      return torch.load(model_weights, map_location='cpu')\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:17:47 save_restore_connector:275] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo.\n"
          ]
        }
      ],
      "source": [
        "citrinet = nemo_asr.models.EncDecCTCModelBPE.from_pretrained('stt_en_citrinet_512')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP4X7KVPli6g",
        "outputId": "f84a59a9-b30c-4202-a17d-c0f5bf45b00f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "  | Name              | Type                              | Params | Mode \n",
              "--------------------------------------------------------------------------------\n",
              "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
              "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
              "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
              "3 | loss              | CTCLoss                           | 0      | train\n",
              "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
              "5 | wer               | WER                               | 0      | train\n",
              "--------------------------------------------------------------------------------\n",
              "37.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "37.0 M    Total params\n",
              "147.977   Total estimated model params size (MB)\n",
              "943       Modules in train mode\n",
              "0         Modules in eval mode"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "citrinet.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB91Swu0pIKr"
      },
      "source": [
        "## Configuración del Modelo usando OmegaConf\n",
        "--------\n",
        "\n",
        "¡Podemos descargar, instanciar y analizar la estructura de alto nivel del modelo `Citrinet` en unas pocas líneas! Ahora profundicemos en el archivo de configuración que hace que el modelo funcione.\n",
        "\n",
        "Primero, importamos [OmegaConf](https://omegaconf.readthedocs.io/en/latest/). OmegaConf es una excelente biblioteca utilizada en todo NeMo que nos permite gestionar configuraciones en formato YAML de manera más sencilla. Además, funciona muy bien con otra biblioteca, [Hydra](https://hydra.cc/docs/intro/), que NeMo utiliza para realizar ediciones de configuración en tiempo real desde la línea de comandos, lo que mejora significativamente la facilidad de uso de nuestros archivos de configuración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkgrDJvumFER"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktakfBluA56"
      },
      "source": [
        "Todos los modelos de NeMo vienen empaquetados con su configuración de modelo dentro del atributo `cfg`. Si bien técnicamente está diseñado para ser una declaración de configuración del modelo tal como se ha construido, `cfg` es una herramienta esencial para modificar el comportamiento del modelo después de su construcción. Esto facilita la realización de muchas tareas esenciales dentro de los modelos.\n",
        "\n",
        "Para mayor seguridad, generalmente trabajamos con una copia de la configuración hasta que estamos listos para editarla dentro del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISd6z7sXt9Mm"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_SiLHRve8A",
        "outputId": "7a11757c-7b0d-4cb1-d9fc-702ad9e4291b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_rate: 16000\n",
            "train_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  trim_silence: true\n",
            "  max_duration: 16.7\n",
            "  shuffle: true\n",
            "  is_tarred: false\n",
            "  tarred_audio_filepaths: null\n",
            "validation_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "test_ds:\n",
            "  manifest_filepath:\n",
            "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "  num_workers: 12\n",
            "  pin_memory: true\n",
            "model_defaults:\n",
            "  repeat: 5\n",
            "  dropout: 0.0\n",
            "  separable: true\n",
            "  se: true\n",
            "  se_context_size: -1\n",
            "tokenizer:\n",
            "  dir: /home/smajumdar/PycharmProjects/nemo-eval/nemo_beta_eval/asrset/manifests/asrset_1.4/tokenizers/no_appen/tokenizer_spe_unigram_v1024/\n",
            "  type: bpe\n",
            "preprocessor:\n",
            "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
            "  sample_rate: 16000\n",
            "  normalize: per_feature\n",
            "  window_size: 0.025\n",
            "  window_stride: 0.01\n",
            "  window: hann\n",
            "  features: 80\n",
            "  n_fft: 512\n",
            "  frame_splicing: 1\n",
            "  dither: 1.0e-05\n",
            "  pad_to: 16\n",
            "  stft_conv: false\n",
            "spec_augment:\n",
            "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
            "  freq_masks: 2\n",
            "  time_masks: 10\n",
            "  freq_width: 27\n",
            "  time_width: 0.05\n",
            "encoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
            "  feat_in: 80\n",
            "  activation: relu\n",
            "  conv_mask: true\n",
            "  jasper:\n",
            "  - filters: 512\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 5\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 11\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 23\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 27\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 29\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 31\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 33\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 35\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 37\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 39\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 640\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 41\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "decoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
            "  feat_in: 640\n",
            "  num_classes: 1024\n",
            "  vocabulary:\n",
            "  - <unk>\n",
            "  - s\n",
            "  - ▁the\n",
            "  - t\n",
            "  - ▁a\n",
            "  - ▁i\n",
            "  - ''''\n",
            "  - ▁and\n",
            "  - ▁to\n",
            "  - ed\n",
            "  - d\n",
            "  - ▁of\n",
            "  - e\n",
            "  - ▁in\n",
            "  - ing\n",
            "  - .\n",
            "  - ▁it\n",
            "  - ▁you\n",
            "  - 'n'\n",
            "  - ▁that\n",
            "  - m\n",
            "  - 'y'\n",
            "  - er\n",
            "  - ▁he\n",
            "  - re\n",
            "  - r\n",
            "  - ▁was\n",
            "  - ▁is\n",
            "  - ▁for\n",
            "  - ▁know\n",
            "  - a\n",
            "  - p\n",
            "  - c\n",
            "  - ','\n",
            "  - ▁be\n",
            "  - o\n",
            "  - ▁but\n",
            "  - ▁they\n",
            "  - g\n",
            "  - ▁so\n",
            "  - ly\n",
            "  - b\n",
            "  - ▁s\n",
            "  - ▁yeah\n",
            "  - ▁we\n",
            "  - ▁have\n",
            "  - ▁re\n",
            "  - ▁like\n",
            "  - l\n",
            "  - ▁on\n",
            "  - ll\n",
            "  - u\n",
            "  - ▁with\n",
            "  - ▁do\n",
            "  - al\n",
            "  - ▁not\n",
            "  - ▁are\n",
            "  - or\n",
            "  - ar\n",
            "  - le\n",
            "  - ▁this\n",
            "  - ▁as\n",
            "  - es\n",
            "  - ▁c\n",
            "  - ▁de\n",
            "  - f\n",
            "  - in\n",
            "  - i\n",
            "  - ve\n",
            "  - ▁uh\n",
            "  - ent\n",
            "  - ▁or\n",
            "  - ▁what\n",
            "  - ▁me\n",
            "  - ▁t\n",
            "  - ▁at\n",
            "  - ▁my\n",
            "  - ▁his\n",
            "  - ▁there\n",
            "  - w\n",
            "  - ▁all\n",
            "  - ▁just\n",
            "  - h\n",
            "  - ▁can\n",
            "  - ri\n",
            "  - il\n",
            "  - k\n",
            "  - ic\n",
            "  - ▁e\n",
            "  - ▁\n",
            "  - ▁um\n",
            "  - ▁don\n",
            "  - ▁b\n",
            "  - ▁had\n",
            "  - ch\n",
            "  - ation\n",
            "  - en\n",
            "  - th\n",
            "  - ▁no\n",
            "  - ▁she\n",
            "  - it\n",
            "  - ▁one\n",
            "  - ▁think\n",
            "  - ▁st\n",
            "  - ▁if\n",
            "  - ▁from\n",
            "  - ter\n",
            "  - ▁an\n",
            "  - an\n",
            "  - ur\n",
            "  - ▁out\n",
            "  - 'on'\n",
            "  - ▁go\n",
            "  - ck\n",
            "  - ▁would\n",
            "  - ▁were\n",
            "  - ▁w\n",
            "  - ▁will\n",
            "  - ▁about\n",
            "  - ▁right\n",
            "  - ment\n",
            "  - ▁her\n",
            "  - te\n",
            "  - ion\n",
            "  - ▁well\n",
            "  - ▁by\n",
            "  - ce\n",
            "  - ▁g\n",
            "  - ▁oh\n",
            "  - ▁up\n",
            "  - ro\n",
            "  - ra\n",
            "  - ▁when\n",
            "  - ▁some\n",
            "  - ▁also\n",
            "  - ▁their\n",
            "  - ers\n",
            "  - ow\n",
            "  - ▁more\n",
            "  - ▁time\n",
            "  - ate\n",
            "  - ▁has\n",
            "  - ▁people\n",
            "  - ▁see\n",
            "  - ▁pa\n",
            "  - el\n",
            "  - ▁get\n",
            "  - ▁ex\n",
            "  - ▁mean\n",
            "  - li\n",
            "  - ▁really\n",
            "  - v\n",
            "  - ▁ra\n",
            "  - ▁been\n",
            "  - ▁said\n",
            "  - '-'\n",
            "  - la\n",
            "  - ge\n",
            "  - ▁how\n",
            "  - ▁po\n",
            "  - ir\n",
            "  - ▁mo\n",
            "  - ▁who\n",
            "  - ▁because\n",
            "  - ▁co\n",
            "  - ▁other\n",
            "  - ▁f\n",
            "  - id\n",
            "  - ol\n",
            "  - ▁un\n",
            "  - ▁now\n",
            "  - ▁work\n",
            "  - ist\n",
            "  - us\n",
            "  - ▁your\n",
            "  - ▁them\n",
            "  - ver\n",
            "  - as\n",
            "  - ne\n",
            "  - ▁ca\n",
            "  - lo\n",
            "  - ▁fa\n",
            "  - ▁him\n",
            "  - ng\n",
            "  - ▁good\n",
            "  - ▁could\n",
            "  - ▁pro\n",
            "  - ive\n",
            "  - ▁con\n",
            "  - de\n",
            "  - un\n",
            "  - age\n",
            "  - ▁ma\n",
            "  - '?'\n",
            "  - at\n",
            "  - ▁ro\n",
            "  - ▁ba\n",
            "  - ▁then\n",
            "  - ▁com\n",
            "  - est\n",
            "  - vi\n",
            "  - ▁dis\n",
            "  - ies\n",
            "  - ance\n",
            "  - ▁su\n",
            "  - ▁even\n",
            "  - ▁any\n",
            "  - ut\n",
            "  - ad\n",
            "  - ul\n",
            "  - ▁se\n",
            "  - ▁two\n",
            "  - ▁bu\n",
            "  - ▁lo\n",
            "  - ▁say\n",
            "  - ▁la\n",
            "  - ▁fi\n",
            "  - is\n",
            "  - ▁li\n",
            "  - ▁over\n",
            "  - ▁new\n",
            "  - ▁man\n",
            "  - ▁sp\n",
            "  - ity\n",
            "  - ▁did\n",
            "  - ▁bo\n",
            "  - ▁very\n",
            "  - x\n",
            "  - end\n",
            "  - ▁which\n",
            "  - ▁our\n",
            "  - ▁after\n",
            "  - ▁o\n",
            "  - ke\n",
            "  - ▁p\n",
            "  - im\n",
            "  - ▁want\n",
            "  - ▁ha\n",
            "  - ▁v\n",
            "  - z\n",
            "  - ▁where\n",
            "  - ard\n",
            "  - um\n",
            "  - ▁into\n",
            "  - ru\n",
            "  - ▁di\n",
            "  - ▁lot\n",
            "  - ▁dr\n",
            "  - mp\n",
            "  - ▁day\n",
            "  - ated\n",
            "  - ci\n",
            "  - ▁these\n",
            "  - ▁than\n",
            "  - ▁take\n",
            "  - ▁kind\n",
            "  - ▁got\n",
            "  - ight\n",
            "  - ▁make\n",
            "  - ence\n",
            "  - ▁pre\n",
            "  - ▁going\n",
            "  - ish\n",
            "  - ▁k\n",
            "  - able\n",
            "  - ▁look\n",
            "  - ti\n",
            "  - per\n",
            "  - ▁here\n",
            "  - ▁en\n",
            "  - ▁ah\n",
            "  - ry\n",
            "  - ▁too\n",
            "  - ▁part\n",
            "  - ant\n",
            "  - one\n",
            "  - ▁ho\n",
            "  - ▁much\n",
            "  - ▁way\n",
            "  - ▁sa\n",
            "  - ▁something\n",
            "  - mo\n",
            "  - ▁us\n",
            "  - ▁th\n",
            "  - ▁mhm\n",
            "  - ▁mi\n",
            "  - ▁off\n",
            "  - pe\n",
            "  - ▁back\n",
            "  - les\n",
            "  - ▁cr\n",
            "  - ▁ri\n",
            "  - ▁fe\n",
            "  - und\n",
            "  - ▁fl\n",
            "  - port\n",
            "  - ▁school\n",
            "  - ▁ch\n",
            "  - ▁should\n",
            "  - ▁first\n",
            "  - ▁only\n",
            "  - ▁le\n",
            "  - ot\n",
            "  - tion\n",
            "  - ▁little\n",
            "  - ▁da\n",
            "  - ▁hu\n",
            "  - ▁d\n",
            "  - me\n",
            "  - ta\n",
            "  - ▁down\n",
            "  - ▁okay\n",
            "  - ▁come\n",
            "  - ain\n",
            "  - ff\n",
            "  - ▁car\n",
            "  - co\n",
            "  - ▁need\n",
            "  - ture\n",
            "  - ▁many\n",
            "  - ▁things\n",
            "  - ▁ta\n",
            "  - qu\n",
            "  - man\n",
            "  - ty\n",
            "  - iv\n",
            "  - ▁year\n",
            "  - he\n",
            "  - ▁thing\n",
            "  - ho\n",
            "  - ▁singapore\n",
            "  - po\n",
            "  - ▁vi\n",
            "  - ▁sc\n",
            "  - ▁still\n",
            "  - der\n",
            "  - ▁hi\n",
            "  - ▁never\n",
            "  - ▁qu\n",
            "  - ia\n",
            "  - ▁fr\n",
            "  - ▁min\n",
            "  - ▁most\n",
            "  - om\n",
            "  - ful\n",
            "  - ▁bi\n",
            "  - ▁long\n",
            "  - ig\n",
            "  - ▁years\n",
            "  - ous\n",
            "  - ▁three\n",
            "  - ▁play\n",
            "  - ▁before\n",
            "  - ▁pi\n",
            "  - ical\n",
            "  - ▁those\n",
            "  - ▁comp\n",
            "  - huh\n",
            "  - ▁live\n",
            "  - tor\n",
            "  - ise\n",
            "  - ▁old\n",
            "  - am\n",
            "  - rr\n",
            "  - ▁sta\n",
            "  - ▁n\n",
            "  - ick\n",
            "  - di\n",
            "  - ma\n",
            "  - ary\n",
            "  - ction\n",
            "  - ▁friend\n",
            "  - ition\n",
            "  - ▁gu\n",
            "  - ▁through\n",
            "  - pp\n",
            "  - for\n",
            "  - ie\n",
            "  - ious\n",
            "  - ▁sh\n",
            "  - ▁home\n",
            "  - lu\n",
            "  - ▁high\n",
            "  - ian\n",
            "  - cu\n",
            "  - ▁help\n",
            "  - ▁give\n",
            "  - ▁talk\n",
            "  - ▁sha\n",
            "  - ▁such\n",
            "  - ▁didn\n",
            "  - em\n",
            "  - ▁may\n",
            "  - ▁ga\n",
            "  - ▁'\n",
            "  - ▁gra\n",
            "  - ▁guess\n",
            "  - ▁every\n",
            "  - ▁app\n",
            "  - tic\n",
            "  - ▁tra\n",
            "  - ▁\"\n",
            "  - op\n",
            "  - ▁made\n",
            "  - '\"'\n",
            "  - ▁op\n",
            "  - ▁own\n",
            "  - ▁mar\n",
            "  - 'no'\n",
            "  - ▁ph\n",
            "  - ▁life\n",
            "  - ▁y\n",
            "  - ak\n",
            "  - ine\n",
            "  - ▁pu\n",
            "  - ▁place\n",
            "  - ▁always\n",
            "  - ▁start\n",
            "  - ▁jo\n",
            "  - ▁pe\n",
            "  - ▁let\n",
            "  - ▁name\n",
            "  - ni\n",
            "  - ▁same\n",
            "  - ▁last\n",
            "  - ▁cl\n",
            "  - ph\n",
            "  - ▁both\n",
            "  - ▁pri\n",
            "  - ities\n",
            "  - ▁another\n",
            "  - and\n",
            "  - ▁al\n",
            "  - ▁boy\n",
            "  - ving\n",
            "  - ▁actually\n",
            "  - ▁person\n",
            "  - ▁went\n",
            "  - ▁yes\n",
            "  - ca\n",
            "  - ally\n",
            "  - ▁h\n",
            "  - ▁great\n",
            "  - ▁thought\n",
            "  - ▁used\n",
            "  - act\n",
            "  - ▁feel\n",
            "  - ward\n",
            "  - ▁different\n",
            "  - ▁cons\n",
            "  - ▁show\n",
            "  - ▁watch\n",
            "  - ▁being\n",
            "  - ▁money\n",
            "  - ay\n",
            "  - ▁try\n",
            "  - ▁why\n",
            "  - ▁big\n",
            "  - ens\n",
            "  - ▁cha\n",
            "  - ▁find\n",
            "  - ▁hand\n",
            "  - ▁real\n",
            "  - ▁four\n",
            "  - ial\n",
            "  - ▁ne\n",
            "  - ▁che\n",
            "  - ▁read\n",
            "  - ▁five\n",
            "  - ▁family\n",
            "  - ag\n",
            "  - ▁change\n",
            "  - ▁add\n",
            "  - ha\n",
            "  - ▁put\n",
            "  - par\n",
            "  - lic\n",
            "  - side\n",
            "  - ▁came\n",
            "  - ▁under\n",
            "  - ness\n",
            "  - ▁per\n",
            "  - j\n",
            "  - ▁around\n",
            "  - ▁end\n",
            "  - ▁house\n",
            "  - if\n",
            "  - ▁while\n",
            "  - vo\n",
            "  - ▁act\n",
            "  - ▁happen\n",
            "  - ▁plan\n",
            "  - mit\n",
            "  - ▁far\n",
            "  - ▁tri\n",
            "  - ▁ten\n",
            "  - ▁du\n",
            "  - ▁win\n",
            "  - ▁tea\n",
            "  - ze\n",
            "  - ▁better\n",
            "  - ▁sure\n",
            "  - ▁mu\n",
            "  - ▁use\n",
            "  - ▁anything\n",
            "  - ▁love\n",
            "  - ▁world\n",
            "  - ▁hard\n",
            "  - ure\n",
            "  - ▁does\n",
            "  - ▁war\n",
            "  - ▁stuff\n",
            "  - ▁ja\n",
            "  - ▁must\n",
            "  - min\n",
            "  - gg\n",
            "  - ▁ru\n",
            "  - ▁care\n",
            "  - ▁tell\n",
            "  - ▁pl\n",
            "  - ▁doing\n",
            "  - ▁probably\n",
            "  - ▁found\n",
            "  - ative\n",
            "  - ▁point\n",
            "  - ach\n",
            "  - ▁ju\n",
            "  - ip\n",
            "  - ▁again\n",
            "  - ▁interest\n",
            "  - ▁state\n",
            "  - ▁week\n",
            "  - na\n",
            "  - ▁might\n",
            "  - ▁pretty\n",
            "  - ▁ki\n",
            "  - ▁fo\n",
            "  - ber\n",
            "  - ▁am\n",
            "  - line\n",
            "  - led\n",
            "  - ▁six\n",
            "  - ▁acc\n",
            "  - ▁bri\n",
            "  - ▁call\n",
            "  - ▁sw\n",
            "  - ▁each\n",
            "  - ▁business\n",
            "  - ▁keep\n",
            "  - ▁away\n",
            "  - cause\n",
            "  - ▁pass\n",
            "  - ▁va\n",
            "  - ▁children\n",
            "  - ▁pay\n",
            "  - ▁count\n",
            "  - ▁public\n",
            "  - ▁everything\n",
            "  - land\n",
            "  - ▁though\n",
            "  - ▁men\n",
            "  - bo\n",
            "  - ▁young\n",
            "  - ▁na\n",
            "  - ▁move\n",
            "  - ough\n",
            "  - ating\n",
            "  - com\n",
            "  - ▁month\n",
            "  - ton\n",
            "  - ▁close\n",
            "  - ▁few\n",
            "  - '!'\n",
            "  - ▁maybe\n",
            "  - ▁imp\n",
            "  - son\n",
            "  - ▁grow\n",
            "  - ▁u\n",
            "  - ▁turn\n",
            "  - ible\n",
            "  - ▁em\n",
            "  - ▁air\n",
            "  - ▁ever\n",
            "  - our\n",
            "  - ▁sea\n",
            "  - ▁fun\n",
            "  - ▁government\n",
            "  - ▁miss\n",
            "  - ▁done\n",
            "  - ▁next\n",
            "  - ▁kids\n",
            "  - ▁cor\n",
            "  - ▁set\n",
            "  - ▁run\n",
            "  - way\n",
            "  - ▁wa\n",
            "  - ▁getting\n",
            "  - ▁eight\n",
            "  - ▁open\n",
            "  - ▁job\n",
            "  - ▁problem\n",
            "  - ook\n",
            "  - ▁night\n",
            "  - ▁learn\n",
            "  - ▁book\n",
            "  - ual\n",
            "  - ▁ti\n",
            "  - ▁best\n",
            "  - cept\n",
            "  - ▁during\n",
            "  - ▁small\n",
            "  - ex\n",
            "  - ▁without\n",
            "  - ▁water\n",
            "  - ▁trans\n",
            "  - ▁course\n",
            "  - ▁once\n",
            "  - ▁sit\n",
            "  - ▁area\n",
            "  - ▁country\n",
            "  - ▁mister\n",
            "  - ▁nothing\n",
            "  - ▁whole\n",
            "  - ▁believe\n",
            "  - ▁service\n",
            "  - ▁took\n",
            "  - ▁face\n",
            "  - ▁bad\n",
            "  - ▁later\n",
            "  - ▁head\n",
            "  - ▁called\n",
            "  - ▁seven\n",
            "  - ▁art\n",
            "  - ▁since\n",
            "  - ▁er\n",
            "  - ▁fact\n",
            "  - ▁city\n",
            "  - ▁market\n",
            "  - ▁hour\n",
            "  - ▁continue\n",
            "  - ship\n",
            "  - ▁invest\n",
            "  - ▁exactly\n",
            "  - ▁large\n",
            "  - ▁true\n",
            "  - ▁nine\n",
            "  - ▁sub\n",
            "  - ▁having\n",
            "  - ▁game\n",
            "  - va\n",
            "  - ▁lu\n",
            "  - ▁conf\n",
            "  - ▁case\n",
            "  - ▁doesn\n",
            "  - ▁certain\n",
            "  - ▁wi\n",
            "  - ▁law\n",
            "  - ▁else\n",
            "  - fi\n",
            "  - ▁left\n",
            "  - ▁enough\n",
            "  - ▁second\n",
            "  - ▁gonna\n",
            "  - ▁food\n",
            "  - ▁hope\n",
            "  - ▁saw\n",
            "  - ▁between\n",
            "  - ▁je\n",
            "  - bi\n",
            "  - ▁girl\n",
            "  - ▁company\n",
            "  - ▁able\n",
            "  - ▁expect\n",
            "  - ▁told\n",
            "  - ▁stand\n",
            "  - ▁group\n",
            "  - ▁main\n",
            "  - ▁walk\n",
            "  - ▁cause\n",
            "  - ▁however\n",
            "  - ▁number\n",
            "  - ▁follow\n",
            "  - ▁near\n",
            "  - ▁yet\n",
            "  - ▁sometimes\n",
            "  - ▁train\n",
            "  - ▁lead\n",
            "  - ▁system\n",
            "  - ▁remain\n",
            "  - ▁develop\n",
            "  - gra\n",
            "  - ▁word\n",
            "  - ▁exc\n",
            "  - ▁together\n",
            "  - ▁consider\n",
            "  - ▁town\n",
            "  - ▁less\n",
            "  - ator\n",
            "  - ▁important\n",
            "  - ▁remember\n",
            "  - ▁free\n",
            "  - ▁quite\n",
            "  - ▁understand\n",
            "  - ▁bra\n",
            "  - ▁support\n",
            "  - ▁idea\n",
            "  - ▁stop\n",
            "  - ▁reason\n",
            "  - ▁nice\n",
            "  - ▁mm\n",
            "  - ▁agree\n",
            "  - ▁low\n",
            "  - ▁against\n",
            "  - ▁issue\n",
            "  - ▁become\n",
            "  - ▁today\n",
            "  - ▁side\n",
            "  - ▁student\n",
            "  - ▁matter\n",
            "  - ▁question\n",
            "  - ▁mother\n",
            "  - ▁father\n",
            "  - ▁hundred\n",
            "  - ▁sort\n",
            "  - ▁eat\n",
            "  - ▁already\n",
            "  - ▁rest\n",
            "  - ▁line\n",
            "  - ▁asked\n",
            "  - ▁include\n",
            "  - ▁upon\n",
            "  - ▁office\n",
            "  - ▁won\n",
            "  - ▁class\n",
            "  - ▁wait\n",
            "  - ▁twenty\n",
            "  - ▁half\n",
            "  - ▁light\n",
            "  - ▁price\n",
            "  - ▁almost\n",
            "  - ash\n",
            "  - ▁child\n",
            "  - ▁sign\n",
            "  - ▁least\n",
            "  - ▁several\n",
            "  - press\n",
            "  - ▁either\n",
            "  - ▁minute\n",
            "  - ▁himself\n",
            "  - ▁parents\n",
            "  - ▁room\n",
            "  - ▁whatever\n",
            "  - ▁general\n",
            "  - ▁cost\n",
            "  - ▁among\n",
            "  - ▁direct\n",
            "  - ▁computer\n",
            "  - ▁appear\n",
            "  - ▁meet\n",
            "  - ▁ski\n",
            "  - ▁return\n",
            "  - ▁couple\n",
            "  - ▁product\n",
            "  - ▁suppose\n",
            "  - ▁definitely\n",
            "  - ▁america\n",
            "  - ▁term\n",
            "  - ▁usually\n",
            "  - ▁strong\n",
            "  - ▁current\n",
            "  - ▁arm\n",
            "  - ▁speak\n",
            "  - ▁local\n",
            "  - ▁south\n",
            "  - ▁experience\n",
            "  - ▁full\n",
            "  - ▁north\n",
            "  - ▁elect\n",
            "  - ▁leave\n",
            "  - ▁provide\n",
            "  - qui\n",
            "  - ▁power\n",
            "  - ▁movie\n",
            "  - ▁everyone\n",
            "  - ▁making\n",
            "  - ▁member\n",
            "  - ▁woman\n",
            "  - ▁somebody\n",
            "  - ▁wonder\n",
            "  - ▁short\n",
            "  - ▁health\n",
            "  - ▁police\n",
            "  - ▁bank\n",
            "  - ▁until\n",
            "  - ▁companies\n",
            "  - ▁everybody\n",
            "  - ▁knew\n",
            "  - ▁program\n",
            "  - ▁music\n",
            "  - ▁york\n",
            "  - ▁land\n",
            "  - ▁doctor\n",
            "  - ▁answer\n",
            "  - ▁building\n",
            "  - ▁employ\n",
            "  - ▁travel\n",
            "  - ▁major\n",
            "  - ▁seems\n",
            "  - ▁safe\n",
            "  - gue\n",
            "  - ▁college\n",
            "  - ▁along\n",
            "  - ▁clear\n",
            "  - ▁especially\n",
            "  - ▁umhu\n",
            "  - ▁result\n",
            "  - ▁type\n",
            "  - ▁court\n",
            "  - ▁black\n",
            "  - ▁hold\n",
            "  - ▁myself\n",
            "  - ▁education\n",
            "  - ▁social\n",
            "  - ▁enjoy\n",
            "  - ▁became\n",
            "  - ▁whether\n",
            "  - ▁morning\n",
            "  - ▁difficult\n",
            "  - ▁shi\n",
            "  - ▁felt\n",
            "  - ▁husband\n",
            "  - ▁white\n",
            "  - ▁taking\n",
            "  - ▁million\n",
            "  - ▁require\n",
            "  - ▁early\n",
            "  - ency\n",
            "  - ▁visit\n",
            "  - ▁level\n",
            "  - ▁brother\n",
            "  - ▁married\n",
            "  - ▁further\n",
            "  - ▁affect\n",
            "  - ▁serve\n",
            "  - ▁present\n",
            "  - ▁park\n",
            "  - ▁effect\n",
            "  - ▁wife\n",
            "  - ▁teacher\n",
            "  - ▁cannot\n",
            "  - ▁community\n",
            "  - ▁street\n",
            "  - ▁period\n",
            "  - ▁national\n",
            "  - ▁view\n",
            "  - ▁future\n",
            "  - ▁daughter\n",
            "  - ▁situation\n",
            "  - ▁grand\n",
            "  - ▁success\n",
            "  - ▁perform\n",
            "  - ▁concern\n",
            "  - ▁complete\n",
            "  - ▁example\n",
            "  - ized\n",
            "  - ▁thousand\n",
            "  - ▁increase\n",
            "  - ▁began\n",
            "  - ▁final\n",
            "  - ▁east\n",
            "  - ▁sense\n",
            "  - ▁charge\n",
            "  - ▁record\n",
            "  - ▁born\n",
            "  - ▁instead\n",
            "  - ▁receive\n",
            "  - ▁women\n",
            "  - ▁across\n",
            "  - ▁information\n",
            "  - ▁although\n",
            "  - ▁process\n",
            "  - ▁condition\n",
            "  - ▁security\n",
            "  - ▁treat\n",
            "  - ▁funny\n",
            "  - ▁custom\n",
            "  - ▁cold\n",
            "  - ▁behind\n",
            "  - ified\n",
            "  - ▁ground\n",
            "  - cycl\n",
            "  - ▁depend\n",
            "  - ▁themselves\n",
            "  - ▁design\n",
            "  - ▁slow\n",
            "  - ▁third\n",
            "  - ▁smoke\n",
            "  - ▁wrong\n",
            "  - ▁project\n",
            "  - ▁space\n",
            "  - ▁drink\n",
            "  - ▁particular\n",
            "  - ▁listen\n",
            "  - ▁thirty\n",
            "  - ▁special\n",
            "  - ability\n",
            "  - ▁improve\n",
            "  - ▁attack\n",
            "  - ▁happy\n",
            "  - ▁strange\n",
            "  - ▁english\n",
            "  - ▁value\n",
            "  - ▁brought\n",
            "  - ▁private\n",
            "  - ▁account\n",
            "  - ▁china\n",
            "  - ▁spoke\n",
            "  - ▁foreign\n",
            "  - ▁possible\n",
            "  - ▁author\n",
            "  - ▁circ\n",
            "  - ▁voice\n",
            "  - ▁figure\n",
            "  - ▁control\n",
            "  - ▁according\n",
            "  - ▁green\n",
            "  - ▁university\n",
            "  - ▁language\n",
            "  - ▁please\n",
            "  - ▁animal\n",
            "  - ▁church\n",
            "  - ▁society\n",
            "  - ▁dream\n",
            "  - ’\n",
            "  - q\n",
            "  - ':'\n",
            "  - ;\n",
            "  - —\n",
            "  - ‘\n",
            "  - ”\n",
            "  - _\n",
            "  - '3'\n",
            "  - '8'\n",
            "  - <\n",
            "  - '>'\n",
            "  - '1'\n",
            "  - –\n",
            "  - '7'\n",
            "  - (\n",
            "  - )\n",
            "  - '0'\n",
            "  - '2'\n",
            "  - '4'\n",
            "  - +\n",
            "  - '&'\n",
            "  - '5'\n",
            "  - '9'\n",
            "  - ü\n",
            "  - é\n",
            "  - /\n",
            "  - á\n",
            "  - ó\n",
            "  - ō\n",
            "  - ú\n",
            "  - ']'\n",
            "  - â\n",
            "  - í\n",
            "  - ã\n",
            "  - ð\n",
            "  - ā\n",
            "  - ć\n",
            "  - č\n",
            "  - š\n",
            "  - è\n",
            "  - ë\n",
            "  - '`'\n",
            "  - ç\n",
            "  - ū\n",
            "  - ạ\n",
            "  - ø\n",
            "  - '='\n",
            "  - à\n",
            "  - ł\n",
            "  - α\n",
            "  - ô\n",
            "  - к\n",
            "  - '}'\n",
            "  - å\n",
            "  - ă\n",
            "  - и\n",
            "  - ī\n",
            "  - π\n",
            "  - œ\n",
            "  - \\\n",
            "  - '['\n",
            "  - ñ\n",
            "  - ß\n",
            "  - ö\n",
            "  - ä\n",
            "  - '6'\n",
            "  - з\n",
            "  - н\n",
            "  - û\n",
            "  - '%'\n",
            "  - '{'\n",
            "  - ¡\n",
            "  - æ\n",
            "  - ê\n",
            "  - þ\n",
            "  - ę\n",
            "  - ě\n",
            "  - ğ\n",
            "  - ń\n",
            "  - ő\n",
            "  - ř\n",
            "  - ž\n",
            "  - ʻ\n",
            "  - в\n",
            "  - е\n",
            "  - й\n",
            "  - л\n",
            "  - ь\n",
            "  - χ\n",
            "  - “\n",
            "optim:\n",
            "  name: novograd\n",
            "  lr: 0.05\n",
            "  betas:\n",
            "  - 0.8\n",
            "  - 0.25\n",
            "  weight_decay: 0.001\n",
            "  sched:\n",
            "    name: CosineAnnealing\n",
            "    warmup_steps: 1000\n",
            "    warmup_ratio: null\n",
            "    min_lr: 1.0e-09\n",
            "    last_epoch: -1\n",
            "target: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE\n",
            "nemo_version: 2.0.0\n",
            "decoding:\n",
            "  strategy: greedy_batch\n",
            "  preserve_alignments: null\n",
            "  compute_timestamps: null\n",
            "  word_seperator: ' '\n",
            "  ctc_timestamp_type: all\n",
            "  batch_dim_index: 0\n",
            "  greedy:\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    preserve_frame_confidence: false\n",
            "    confidence_method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  beam:\n",
            "    beam_size: 4\n",
            "    search_type: default\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    return_best_hypothesis: true\n",
            "    beam_alpha: 1.0\n",
            "    beam_beta: 0.0\n",
            "    kenlm_path: null\n",
            "    flashlight_cfg:\n",
            "      lexicon_path: null\n",
            "      boost_path: null\n",
            "      beam_size_token: 16\n",
            "      beam_threshold: 20.0\n",
            "      unk_weight: -.inf\n",
            "      sil_weight: 0.0\n",
            "    pyctcdecode_cfg:\n",
            "      beam_prune_logp: -10.0\n",
            "      token_min_logp: -5.0\n",
            "      prune_history: false\n",
            "      hotwords: null\n",
            "      hotword_weight: 10.0\n",
            "  wfst:\n",
            "    beam_size: 4\n",
            "    search_type: riva\n",
            "    return_best_hypothesis: true\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    decoding_mode: nbest\n",
            "    open_vocabulary_decoding: false\n",
            "    beam_width: 10.0\n",
            "    lm_weight: 1.0\n",
            "    device: cuda\n",
            "    arpa_lm_path: null\n",
            "    wfst_lm_path: null\n",
            "    riva_decoding_cfg: {}\n",
            "    k2_decoding_cfg:\n",
            "      search_beam: 20.0\n",
            "      output_beam: 10.0\n",
            "      min_active_states: 30\n",
            "      max_active_states: 10000\n",
            "  confidence_cfg:\n",
            "    preserve_frame_confidence: false\n",
            "    preserve_token_confidence: false\n",
            "    preserve_word_confidence: false\n",
            "    exclude_blank: true\n",
            "    aggregation: min\n",
            "    tdt_include_duration: false\n",
            "    method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  temperature: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cfg = copy.deepcopy(citrinet.cfg)\n",
        "print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_V3e3W7vqOb"
      },
      "source": [
        "## Analizando el contenido de la configuración del modelo\n",
        "----------\n",
        "\n",
        "Arriba vemos una configuración para el modelo Citrinet. Como se discutió al principio, los modelos de NeMo contienen la definición completa de la(s) red(es) neuronal(es), así como la mayor parte de la infraestructura circundante para soportar ese modelo dentro de sí mismos. Aquí vemos un ejemplo perfecto de este comportamiento.\n",
        "\n",
        "La configuración de Citrinet incluye:\n",
        "\n",
        "- **`preprocessor`**: Capa de preprocesamiento de MelSpectrogram.\n",
        "- **`encoder`**: El modelo de codificador acústico.\n",
        "- **`decoder`**: La capa de decodificador CTC.\n",
        "- **`optim`** (y potencialmente **`sched`**): Configuración del optimizador. Opcionalmente, puede incluir información sobre el programador de tasas de aprendizaje.\n",
        "- **`spec_augment`**: Soporte para la mejora de espectrogramas.\n",
        "- **`train_ds`**, **`validation_ds`** y **`test_ds`**: Información para la construcción de conjuntos de datos y cargadores de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIwhdXkwxn6R"
      },
      "source": [
        "## Modificando el contenido de la configuración del modelo\n",
        "----------\n",
        "\n",
        "Supongamos que queremos experimentar con un preprocesador diferente (queremos MelSpectrogram, pero con una configuración diferente a la proporcionada en la configuración original). O supongamos que queremos agregar un programador a este modelo durante el entrenamiento.\n",
        "\n",
        "¡OmegaConf hace que esta tarea sea muy sencilla para nosotros!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlSZ8EA4yGKo",
        "outputId": "24f0fe5a-4083-4f9f-f97a-2dd3f55572cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Old Config: \n",
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  warmup_ratio: null\n",
            "  min_lr: 1.0e-09\n",
            "  last_epoch: -1\n",
            "\n",
            "New Config: \n",
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  min_lr: 1.0e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# OmegaConf won't allow you to add new config items, so we temporarily disable this safeguard.\n",
        "OmegaConf.set_struct(cfg, False)\n",
        "\n",
        "# Let's see the old optim config\n",
        "print(\"Old Config: \")\n",
        "print(OmegaConf.to_yaml(cfg.optim))\n",
        "\n",
        "sched = {'name': 'CosineAnnealing', 'warmup_steps': 1000, 'min_lr': 1e-6}\n",
        "sched = OmegaConf.create(sched)  # Convert it into a DictConfig\n",
        "\n",
        "# Assign it to cfg.optim.sched namespace\n",
        "cfg.optim.sched = sched\n",
        "\n",
        "# Let's see the new optim config\n",
        "print(\"New Config: \")\n",
        "print(OmegaConf.to_yaml(cfg.optim))\n",
        "\n",
        "# Here, we restore the safeguards so no more additions can be made to the config\n",
        "OmegaConf.set_struct(cfg, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMDN66502kn"
      },
      "source": [
        "## Actualizando el modelo desde la configuración\n",
        "----------\n",
        "\n",
        "Los modelos de NeMo se pueden actualizar de varias maneras, pero seguimos patrones similares dentro de cada colección para mantener la coherencia.\n",
        "\n",
        "Aquí mostraremos las dos formas más comunes de modificar los componentes principales del modelo: utilizando el método `from_config_dict` y actualizando algunas partes específicas del modelo.\n",
        "\n",
        "Recuerda que todos los modelos de NeMo son módulos de PyTorch Lightning, los cuales a su vez son módulos de PyTorch, ¡así que tenemos mucha flexibilidad aquí!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrKzFYkZ20aa"
      },
      "source": [
        "### Actualizar el modelo utilizando `from_config_dict`\n",
        "\n",
        "En ciertos archivos de configuración, notarás el siguiente patrón:\n",
        "\n",
        "```yaml\n",
        "preprocessor:\n",
        "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
        "  normalize: per_feature\n",
        "  window_size: 0.02\n",
        "  sample_rate: 16000\n",
        "  window_stride: 0.01\n",
        "  window: hann\n",
        "  features: 64\n",
        "  n_fft: 512\n",
        "  frame_splicing: 1\n",
        "  dither: 1.0e-05\n",
        "  stft_conv: false\n",
        "```\n",
        "\n",
        "Podrías preguntarte, ¿por qué usamos `_target_`? Bueno, en general es poco común que el preprocesador, el codificador, el decodificador y quizás algunos otros detalles se cambien con frecuencia desde la línea de comandos al experimentar. Para estabilizar estas configuraciones, imponemos que nuestro preprocesador siempre sea del tipo `AudioToMelSpectrogramPreprocessor` para este modelo al establecer su atributo `_target_` en la configuración. Para proporcionar sus parámetros en el constructor de la clase, simplemente los agregamos después de `_target_`.\n",
        "\n",
        "---------\n",
        "Nota: aún podemos cambiar todos los parámetros de esta clase `AudioToMelSpectrogramPreprocessor` desde la línea de comandos utilizando Hydra, por lo que no perdemos flexibilidad una vez que decidimos qué tipo de clase de preprocesamiento queremos.\n",
        "\n",
        "¡Esto también nos proporciona una forma flexible de instanciar partes del modelo solo a partir del objeto de configuración!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "1Be08R4szkT3",
        "outputId": "1902b411-0091-4dbf-c44b-71cf648e059f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'copy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f3d62b8847d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_preprocessor_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_preprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcitrinet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_preprocessor_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
          ]
        }
      ],
      "source": [
        "new_preprocessor_config = copy.deepcopy(cfg.preprocessor)\n",
        "new_preprocessor = citrinet.from_config_dict(new_preprocessor_config)\n",
        "print(new_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzJQ7Y8H4S_U"
      },
      "source": [
        "Entonces, ¿cómo actualizamos realmente el preprocesador interno de nuestro modelo con algo nuevo? Bueno, dado que los modelos de NeMo son simplemente módulos de PyTorch, ¡podemos reemplazar directamente su atributo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdtnPKX84OJ-"
      },
      "outputs": [],
      "source": [
        "citrinet.preprocessor = new_preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMz2KR-24xTO",
        "outputId": "88cee44b-ef2a-45bd-c148-e9992a3185e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "  | Name              | Type                              | Params | Mode \n",
              "--------------------------------------------------------------------------------\n",
              "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
              "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
              "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
              "3 | loss              | CTCLoss                           | 0      | train\n",
              "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
              "5 | wer               | WER                               | 0      | train\n",
              "--------------------------------------------------------------------------------\n",
              "37.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "37.0 M    Total params\n",
              "147.977   Total estimated model params size (MB)\n",
              "943       Modules in train mode\n",
              "0         Modules in eval mode"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "citrinet.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPb_BdPN40Ro"
      },
      "source": [
        "--------\n",
        "Esto podría parecer que no cambió nada, ¡porque en realidad no modificamos la configuración del preprocesador en absoluto! Pero, como mostramos anteriormente, podemos modificar fácilmente la configuración del preprocesador, instanciarlo desde la configuración y luego simplemente asignarlo al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8WKJkD5E_Q"
      },
      "source": [
        "-------\n",
        "**NOTA**: Los preprocesadores generalmente no tienen pesos, por lo que esto fue sencillo, pero, ¿qué pasa si queremos reemplazar una parte del modelo que realmente tiene parámetros entrenados?\n",
        "\n",
        "Bueno, el enfoque anterior seguirá funcionando, solo recuerda que el nuevo módulo que insertes en `citrinet.encoder` o `citrinet.decoder` no tendrá pesos preentrenados. Sin embargo, puedes solucionar esto fácilmente cargando el estado del diccionario del módulo *antes* de asignarlo al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YplQcgfG6S1U"
      },
      "source": [
        "### Preservar la nueva configuración\n",
        "\n",
        "Hemos actualizado el preprocesador del modelo. Sin embargo, también necesitamos realizar un paso crucial: **preservar la configuración actualizada**.\n",
        "\n",
        "¿Por qué queremos hacer esto? NeMo tiene muchas formas de guardar y restaurar sus modelos, las cuales discutiremos más adelante. Todas ellas dependen de tener una configuración actualizada que defina el modelo en su totalidad, por lo que si modificamos algo, también debemos actualizar la parte correspondiente de la configuración para guardar y restaurar los modelos de manera segura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsxQHBV86R4a"
      },
      "outputs": [],
      "source": [
        "# Update the config copy\n",
        "cfg.preprocessor = new_preprocessor_config\n",
        "# Update the model config\n",
        "citrinet.cfg = cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXRRBnJk5tCv"
      },
      "source": [
        "## Actualizar algunos componentes especiales del modelo\n",
        "---------\n",
        "\n",
        "Aunque el enfoque anterior es útil para la mayoría de los componentes principales del modelo, NeMo tiene utilidades especiales para algunos componentes específicos.\n",
        "\n",
        "Estas utilidades son:\n",
        "\n",
        " - `setup_training_data`\n",
        " - `setup_validation_data` y `setup_multi_validation_data`\n",
        " - `setup_test_data` y `setup_multi_test_data`\n",
        " - `setup_optimization`\n",
        "\n",
        "Estas utilidades especiales están diseñadas para ayudarte a configurar fácilmente el entrenamiento, la validación y las pruebas una vez que hayas restaurado un modelo desde un punto de control.\n",
        "\n",
        "------\n",
        "Una de las principales tareas de todos los modelos de IA conversacional es el ajuste fino en nuevos conjuntos de datos: nuevos idiomas, nuevos corpus de texto, nuevas voces, etc. A menudo no es suficiente contar solo con un modelo preentrenado. Por ello, se proporcionan estos métodos de configuración para permitir a los usuarios adaptar los modelos *después* de que ya han sido entrenados o entregados.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Y7wt2x9goJ"
      },
      "source": [
        "Es posible que recuerdes haber visto algunos mensajes de advertencia en el momento en que intentaste instanciar el modelo preentrenado. Esas advertencias, de hecho, son recordatorios para llamar a los métodos de configuración adecuados para la tarea que deseas realizar.\n",
        "\n",
        "Esas advertencias simplemente muestran la configuración antigua que se utilizó para entrenar ese modelo y son una plantilla básica que puedes modificar fácilmente. Tienes la capacidad de modificar las subconfiguraciones `train_ds`, `validation_ds` y `test_ds` en su totalidad para evaluar, ajustar o entrenar el modelo desde cero, o para cualquier otro propósito que necesites.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hXXdaup-QmG"
      },
      "source": [
        "Hablemos de cómo agregar un programador al modelo a continuación (que inicialmente solo tenía un optimizador en su configuración)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cveKWvMZ4zBo",
        "outputId": "aa2451bc-51ec-4d5f-a737-1372d9f84425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name: novograd\n",
            "lr: 0.05\n",
            "betas:\n",
            "- 0.8\n",
            "- 0.25\n",
            "weight_decay: 0.001\n",
            "sched:\n",
            "  name: CosineAnnealing\n",
            "  warmup_steps: 1000\n",
            "  min_lr: 1.0e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's print out the current optimizer\n",
        "print(OmegaConf.to_yaml(citrinet.cfg.optim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVguw3k0-f6b",
        "outputId": "94721d58-8843-4ca0-f7d6-cf93eef1c115"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:19:21 modelPT:666] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:19:21 modelPT:787] Optimizer config = Novograd (\n",
            "    Parameter Group 0\n",
            "        amsgrad: False\n",
            "        betas: [0.8, 0.25]\n",
            "        eps: 1e-08\n",
            "        grad_averaging: False\n",
            "        lr: 0.05\n",
            "        weight_decay: 0.001\n",
            "    )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:19:21 lr_scheduler:928] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
            "    Scheduler will not be instantiated !\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Novograd (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: [0.8, 0.25]\n",
              "     eps: 1e-08\n",
              "     grad_averaging: False\n",
              "     lr: 0.05\n",
              "     weight_decay: 0.001\n",
              " ),\n",
              " None)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now let's update the config\n",
        "citrinet.setup_optimization(cfg.optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZBCQeW-21X"
      },
      "source": [
        "-------\n",
        "Vemos una advertencia:\n",
        "\n",
        "```\n",
        "Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
        "    Scheduler will not be instantiated !\n",
        "```\n",
        "\n",
        "No tenemos un conjunto de datos de entrenamiento configurado, ni tampoco `max_steps` en la configuración. ¡La mayoría de los programadores de NeMo no pueden ser instanciados sin calcular cuántos pasos de entrenamiento existen realmente!\n",
        "\n",
        "Aquí, podemos permitir temporalmente la construcción del programador pasando explícitamente un valor para `max_steps`, como 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqC89hfE-tqf"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(cfg.optim.sched, False)\n",
        "\n",
        "cfg.optim.sched.max_steps = 100\n",
        "\n",
        "OmegaConf.set_struct(cfg.optim.sched, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r22IqOBK_q6l",
        "outputId": "3ee9164c-9bad-4b92-a9a9-fdf5cdd319ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:19:28 modelPT:666] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:19:28 modelPT:787] Optimizer config = Novograd (\n",
            "    Parameter Group 0\n",
            "        amsgrad: False\n",
            "        betas: [0.8, 0.25]\n",
            "        eps: 1e-08\n",
            "        grad_averaging: False\n",
            "        lr: 0.05\n",
            "        weight_decay: 0.001\n",
            "    )\n",
            "[NeMo I 2024-12-24 08:19:28 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7c332271ea40>\" \n",
            "    will be used during training (effective maximum steps = 100) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 1000\n",
            "    min_lr: 1.0e-06\n",
            "    max_steps: 100\n",
            "    )\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Novograd (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: [0.8, 0.25]\n",
              "     eps: 1e-08\n",
              "     grad_averaging: False\n",
              "     initial_lr: 0.05\n",
              "     lr: 4.995004995004995e-05\n",
              "     weight_decay: 0.001\n",
              " ),\n",
              " {'scheduler': <nemo.core.optim.lr_scheduler.CosineAnnealing at 0x7c332271ea40>,\n",
              "  'interval': 'step',\n",
              "  'frequency': 1,\n",
              "  'monitor': 'loss',\n",
              "  'reduce_on_plateau': False})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now let's update the config and try again\n",
        "citrinet.setup_optimization(cfg.optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Eezf_sAVS0"
      },
      "source": [
        "Podrías preguntarte por qué no configuramos explícitamente `citrinet.cfg.optim = cfg.optim`.\n",
        "\n",
        "¡Esto se debe a que el método `setup_optimization()` lo hace por ti! Aún puedes actualizar la configuración manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THqhXy_lQ7i8"
      },
      "source": [
        "### Configuración del Optimizador y el Planificador\n",
        "\n",
        "Los optimizadores y los planificadores son componentes comunes de los modelos y son esenciales para entrenar el modelo desde cero.\n",
        "\n",
        "Están agrupados bajo un espacio de nombres unificado `optim`, ya que los planificadores a menudo operan sobre un optimizador dado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HY51nuoSJs5"
      },
      "source": [
        "### Desglosemos la estructura de `optim`\n",
        "```yaml\n",
        "optim:\n",
        "    name: novograd\n",
        "    lr: 0.01\n",
        "\n",
        "    # optimizer arguments\n",
        "    betas: [0.8, 0.25]\n",
        "    weight_decay: 0.001\n",
        "\n",
        "    # scheduler setup\n",
        "    sched:\n",
        "      name: CosineAnnealing\n",
        "\n",
        "      # Optional arguments\n",
        "      max_steps: -1 # computed at runtime or explicitly set here\n",
        "      monitor: val_loss\n",
        "      reduce_on_plateau: false\n",
        "\n",
        "      # scheduler config override\n",
        "      warmup_steps: 1000\n",
        "      warmup_ratio: null\n",
        "      min_lr: 1e-9\n",
        "```\n",
        "Componentes esenciales del optimizador -\n",
        "\n",
        " - `name`: Nombre del optimizador como cadena de texto. Generalmente es el nombre de la clase en minúsculas.\n",
        " - `lr`: La tasa de aprendizaje es un argumento requerido para todos los optimizadores.\n",
        "\n",
        "Componentes opcionales del optimizador - después de proporcionar los dos argumentos anteriores, cualquier argumento adicional agregado bajo `optim` se pasará al constructor de ese optimizador como argumentos de palabra clave.\n",
        "\n",
        " - `betas`: Lista de valores beta para pasar al optimizador.\n",
        " - `weight_decay`: Decaimiento de peso opcional pasado al optimizador.\n",
        "\n",
        "Componentes opcionales del programador - `sched` es una configuración opcional del programador para el optimizador dado.\n",
        "\n",
        "Si se proporciona `sched`, solo se necesita un argumento esencial:\n",
        "\n",
        " - `name`: El nombre del programador. Generalmente, es el nombre completo de la clase.\n",
        "\n",
        "Componentes opcionales del programador -\n",
        "\n",
        " - `max_steps`: Máximo de pasos como una anulación del usuario. Si se proporciona `trainer.max_steps` dentro de la configuración del entrenador, se usa ese valor. Si no se establece ningún valor, el programador intentará calcular los `max_steps efectivos` utilizando el tamaño del cargador de datos de entrenamiento. Si eso también falla, entonces el programador no se creará en absoluto.\n",
        "\n",
        " - `monitor`: Se usa si estás utilizando un programador adaptativo como ReduceLROnPlateau. De lo contrario, se ignora. Por defecto es `loss`, indicando la pérdida de entrenamiento como monitor.\n",
        "\n",
        " - `reduce_on_plateau`: Necesario establecer en verdadero si se utiliza un programador adaptativo.\n",
        "\n",
        "Cualquier argumento adicional bajo `sched` se suministrará como argumentos de palabra clave al constructor del programador.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3pQM2aj_6WX"
      },
      "source": [
        "## Diferencia entre los métodos de configuración del cargador de datos\n",
        "----------\n",
        "\n",
        "Podrás notar que tenemos múltiples métodos de configuración para los conjuntos de datos de validación y prueba. También notarás que no tenemos un equivalente `setup_multi_train_data`.\n",
        "\n",
        "En general, los métodos `multi` se refieren a múltiples conjuntos de datos / cargadores de datos.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g33nMx9WCJdj"
      },
      "source": [
        "### ¿Dónde está `setup_multi_train_data`?\n",
        "Con lo anterior en mente, abordemos por qué no tenemos `setup_multi_train_data`.\n",
        "\n",
        "NeMo se ocupa de múltiples dominios: `asr`, `nlp` y `tts`. La forma en que se configuran y utilizan los conjuntos de datos en estos dominios es dramáticamente diferente. A menudo no está claro qué significa tener múltiples conjuntos de datos de entrenamiento: ¿los concatenamos? ¿Los muestreamos aleatoriamente (con la misma o diferente probabilidad) de cada uno de ellos?\n",
        "\n",
        "Por lo tanto, dejamos ese soporte para múltiples conjuntos de datos al propio modelo. Por ejemplo, en ASR, ¡puedes concatenar múltiples archivos de manifiesto de entrenamiento utilizando comas al proporcionar el valor de `manifest_filepath`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjI2Q5LECJib"
      },
      "source": [
        "### ¿Qué son los métodos multi?\n",
        "\n",
        "En muchos casos, especialmente en ASR y NLP, podemos tener múltiples conjuntos de datos de validación y prueba. El ejemplo más común de esto en ASR es `Librispeech`, que tiene `dev_clean`, `dev_other`, `test_clean`, `test_other`.\n",
        "\n",
        "NeMo estandariza cómo manejar múltiples cargadores de datos para validación y prueba, de modo que todas nuestras colecciones tengan una apariencia y sensación similar, así como facilitar el desarrollo de nuestros modelos. Durante la evaluación, estos conjuntos de datos se tratan de manera independiente y se les anteponen nombres resueltos para que los registros sean separados.\n",
        "\n",
        "Por lo tanto, los métodos `multi` son generalizaciones de los métodos de configuración de datos de validación y prueba únicos, con alguna funcionalidad adicional. Si proporcionas múltiples conjuntos de datos, aún tienes que escribir código para solo un conjunto de datos y NeMo automáticamente adjuntará los nombres apropiados a tus registros para que puedas diferenciarlos.\n",
        "\n",
        "Además, también preservan automáticamente la configuración que el usuario les pasa al actualizar los cargadores de datos de validación o prueba.\n",
        "\n",
        "**En general, se prefiere llamar a los métodos `setup_multi_validation_data` y `setup_multi_test_data`, incluso si solo estás utilizando conjuntos de datos únicos, simplemente por la gestión automatizada que proporcionan.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKURHn0jH_52"
      },
      "source": [
        "## Creación del Modelo desde el constructor vs restaurar un modelo\n",
        "---------\n",
        "\n",
        "Podrías notar que discutimos todos los métodos de configuración anteriores en el contexto del modelo después de que se ha restaurado. Sin embargo, los scripts de NeMo no los llaman dentro de ninguno de los scripts de entrenamiento de ejemplo.\n",
        "\n",
        "Esto se debe a que estos métodos son llamados automáticamente por el constructor cuando el Modelo se crea por primera vez, pero estos métodos se omiten durante la restauración (ya sea desde un punto de control de PyTorch Lightning usando `load_from_checkpoint`, o mediante el método `restore_from` dentro de los Modelos de NeMo).\n",
        "\n",
        "Esto se hace ya que la mayoría de los conjuntos de datos se almacenan en el directorio local de un usuario, y la ruta a estos conjuntos de datos se establece en la configuración (ya sea establecida por defecto, o establecida por sobrescrituras de Hydra). Por otro lado, los modelos están destinados a ser portátiles. En el sistema de otro usuario, los datos podrían no estar ubicados exactamente en el mismo lugar, o incluso en la misma unidad especificada en la configuración del modelo.\n",
        "\n",
        "Por lo tanto, permitimos que el constructor tenga cierta flexibilidad y automatice dicha configuración de conjuntos de datos, mientras que la restauración advierte que los cargadores de datos no se configuraron y proporciona al usuario formas de configurar sus propios conjuntos de datos.\n",
        "\n",
        "------\n",
        "\n",
        "¿Por qué los optimizadores no se restauran automáticamente? Bueno, los optimizadores en sí mismos no enfrentan un problema, pero como vimos antes, los planificadores dependen del número de pasos de entrenamiento para calcular su programación.\n",
        "\n",
        "Sin embargo, si no deseas modificar el optimizador y el planificador, y prefieres dejarlos en sus valores predeterminados, está perfectamente bien. ¡El método `setup_optimization()` es llamado automáticamente por PyTorch Lightning para ti cuando comienzas a entrenar tu modelo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g91FE8mlMcnh"
      },
      "source": [
        "## Guardar y restaurar modelos\n",
        "----------\n",
        "\n",
        "NeMo proporciona varias formas de guardar y restaurar modelos. Si utilizas el Experiment Manager que forma parte de todos los scripts de entrenamiento de NeMo, PyTorch Lightning guardará automáticamente puntos de control en el directorio del experimento.\n",
        "\n",
        "También podemos usar archivos empaquetados utilizando los métodos especializados `save_to` y `restore_from`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzMxga7QNYn8"
      },
      "source": [
        "### Guardar y Restaurar desde Checkpoints de PTL\n",
        "----------\n",
        "\n",
        "The PyTorch Lightning Trainer object will periodically save checkpoints when the experiment manager is being used during training.\n",
        "\n",
        "PyTorch Lightning checkpoints can then be loaded and evaluated / fine-tuned just as always using the class method `load_from_checkpoint`.\n",
        "\n",
        "For example, restore a Citrinet model from a checkpoint -\n",
        "\n",
        "```python\n",
        "citrinet = nemo_asr.models.EncDecCTCModelBPE.load_from_checkpoint(<path to checkpoint>)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4YzAG-KOBkZ"
      },
      "source": [
        "### Guardar y Restaurar desde archivos .nemo\n",
        "----------\n",
        "\n",
        "Hay algunos modelos que pueden requerir dependencias externas para ser empaquetadas con ellos para restaurarlos correctamente.\n",
        "\n",
        "Un ejemplo de esto es un modelo ASR con un tokenizador BPE externo. Es preferible que el modelo incluya todos los componentes necesarios para restaurarlo, pero un archivo binario para un tokenizador no puede ser serializado en un punto de control de PyTorch Lightning.\n",
        "\n",
        "En tales casos, podemos usar el método `save_to` y `restore_from` para empaquetar todo el modelo y sus componentes (aquí, los archivos del tokenizador) en un archivo tar. Esto luego puede ser fácilmente importado por el usuario y utilizado para restaurar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6_vMSwXNJ74"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "citrinet.save_to('citrinet_512.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrBhgaqyP4rU",
        "outputId": "716f2db1-cfe7-4001-9a1f-9e941a06d81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "citrinet_512.nemo\n"
          ]
        }
      ],
      "source": [
        "!ls -d -- *.nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyht1E0DQGb_",
        "outputId": "82479645-e814-4fa5-f047-b26a452bc433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:19:48 mixins:173] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:19:49 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    trim_silence: true\n",
            "    max_duration: 16.7\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    \n",
            "[NeMo W 2024-12-24 08:19:49 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2024-12-24 08:19:49 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 12\n",
            "    pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:19:49 features:305] PADDING: 16\n",
            "[NeMo I 2024-12-24 08:19:51 save_restore_connector:275] Model EncDecCTCModelBPE was successfully restored from /content/citrinet_512.nemo.\n"
          ]
        }
      ],
      "source": [
        "# Restore the model\n",
        "temp_cn = nemo_asr.models.EncDecCTCModelBPE.restore_from('citrinet_512.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqNpmYYJQS2H",
        "outputId": "3c203bdd-e4d7-40ab-8694-a55c41888924"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "  | Name              | Type                              | Params | Mode \n",
              "--------------------------------------------------------------------------------\n",
              "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
              "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
              "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
              "3 | loss              | CTCLoss                           | 0      | train\n",
              "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
              "5 | wer               | WER                               | 0      | train\n",
              "--------------------------------------------------------------------------------\n",
              "37.0 M    Trainable params\n",
              "0         Non-trainable params\n",
              "37.0 M    Total params\n",
              "147.977   Total estimated model params size (MB)\n",
              "943       Modules in train mode\n",
              "0         Modules in eval mode"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp_cn.summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5e42EoiZYjf",
        "outputId": "fd652beb-de15-4ec7-adbc-5a8c46eeee1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_rate: 16000\n",
            "train_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  trim_silence: true\n",
            "  max_duration: 16.7\n",
            "  shuffle: true\n",
            "  is_tarred: false\n",
            "  tarred_audio_filepaths: null\n",
            "validation_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "test_ds:\n",
            "  manifest_filepath:\n",
            "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
            "  sample_rate: 16000\n",
            "  batch_size: 32\n",
            "  shuffle: false\n",
            "  num_workers: 12\n",
            "  pin_memory: true\n",
            "model_defaults:\n",
            "  repeat: 5\n",
            "  dropout: 0.0\n",
            "  separable: true\n",
            "  se: true\n",
            "  se_context_size: -1\n",
            "tokenizer:\n",
            "  dir: /home/smajumdar/PycharmProjects/nemo-eval/nemo_beta_eval/asrset/manifests/asrset_1.4/tokenizers/no_appen/tokenizer_spe_unigram_v1024/\n",
            "  type: bpe\n",
            "  model_path: nemo:tokenizer.model\n",
            "  vocab_path: nemo:vocab.txt\n",
            "preprocessor:\n",
            "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
            "  sample_rate: 16000\n",
            "  normalize: per_feature\n",
            "  window_size: 0.025\n",
            "  window_stride: 0.01\n",
            "  window: hann\n",
            "  features: 80\n",
            "  n_fft: 512\n",
            "  frame_splicing: 1\n",
            "  dither: 1.0e-05\n",
            "  pad_to: 16\n",
            "  stft_conv: false\n",
            "spec_augment:\n",
            "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
            "  freq_masks: 2\n",
            "  time_masks: 10\n",
            "  freq_width: 27\n",
            "  time_width: 0.05\n",
            "encoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
            "  feat_in: 80\n",
            "  activation: relu\n",
            "  conv_mask: true\n",
            "  jasper:\n",
            "  - filters: 512\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 5\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 11\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 19\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 21\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 23\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 25\n",
            "    stride:\n",
            "    - 2\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "    stride_last: true\n",
            "    residual_mode: stride_add\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 27\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 29\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 31\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 33\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 35\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 37\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 512\n",
            "    repeat: 5\n",
            "    kernel:\n",
            "    - 39\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "  - filters: 640\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 41\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    se: true\n",
            "    se_context_size: -1\n",
            "decoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
            "  feat_in: 640\n",
            "  num_classes: 1024\n",
            "  vocabulary:\n",
            "  - <unk>\n",
            "  - s\n",
            "  - ▁the\n",
            "  - t\n",
            "  - ▁a\n",
            "  - ▁i\n",
            "  - ''''\n",
            "  - ▁and\n",
            "  - ▁to\n",
            "  - ed\n",
            "  - d\n",
            "  - ▁of\n",
            "  - e\n",
            "  - ▁in\n",
            "  - ing\n",
            "  - .\n",
            "  - ▁it\n",
            "  - ▁you\n",
            "  - 'n'\n",
            "  - ▁that\n",
            "  - m\n",
            "  - 'y'\n",
            "  - er\n",
            "  - ▁he\n",
            "  - re\n",
            "  - r\n",
            "  - ▁was\n",
            "  - ▁is\n",
            "  - ▁for\n",
            "  - ▁know\n",
            "  - a\n",
            "  - p\n",
            "  - c\n",
            "  - ','\n",
            "  - ▁be\n",
            "  - o\n",
            "  - ▁but\n",
            "  - ▁they\n",
            "  - g\n",
            "  - ▁so\n",
            "  - ly\n",
            "  - b\n",
            "  - ▁s\n",
            "  - ▁yeah\n",
            "  - ▁we\n",
            "  - ▁have\n",
            "  - ▁re\n",
            "  - ▁like\n",
            "  - l\n",
            "  - ▁on\n",
            "  - ll\n",
            "  - u\n",
            "  - ▁with\n",
            "  - ▁do\n",
            "  - al\n",
            "  - ▁not\n",
            "  - ▁are\n",
            "  - or\n",
            "  - ar\n",
            "  - le\n",
            "  - ▁this\n",
            "  - ▁as\n",
            "  - es\n",
            "  - ▁c\n",
            "  - ▁de\n",
            "  - f\n",
            "  - in\n",
            "  - i\n",
            "  - ve\n",
            "  - ▁uh\n",
            "  - ent\n",
            "  - ▁or\n",
            "  - ▁what\n",
            "  - ▁me\n",
            "  - ▁t\n",
            "  - ▁at\n",
            "  - ▁my\n",
            "  - ▁his\n",
            "  - ▁there\n",
            "  - w\n",
            "  - ▁all\n",
            "  - ▁just\n",
            "  - h\n",
            "  - ▁can\n",
            "  - ri\n",
            "  - il\n",
            "  - k\n",
            "  - ic\n",
            "  - ▁e\n",
            "  - ▁\n",
            "  - ▁um\n",
            "  - ▁don\n",
            "  - ▁b\n",
            "  - ▁had\n",
            "  - ch\n",
            "  - ation\n",
            "  - en\n",
            "  - th\n",
            "  - ▁no\n",
            "  - ▁she\n",
            "  - it\n",
            "  - ▁one\n",
            "  - ▁think\n",
            "  - ▁st\n",
            "  - ▁if\n",
            "  - ▁from\n",
            "  - ter\n",
            "  - ▁an\n",
            "  - an\n",
            "  - ur\n",
            "  - ▁out\n",
            "  - 'on'\n",
            "  - ▁go\n",
            "  - ck\n",
            "  - ▁would\n",
            "  - ▁were\n",
            "  - ▁w\n",
            "  - ▁will\n",
            "  - ▁about\n",
            "  - ▁right\n",
            "  - ment\n",
            "  - ▁her\n",
            "  - te\n",
            "  - ion\n",
            "  - ▁well\n",
            "  - ▁by\n",
            "  - ce\n",
            "  - ▁g\n",
            "  - ▁oh\n",
            "  - ▁up\n",
            "  - ro\n",
            "  - ra\n",
            "  - ▁when\n",
            "  - ▁some\n",
            "  - ▁also\n",
            "  - ▁their\n",
            "  - ers\n",
            "  - ow\n",
            "  - ▁more\n",
            "  - ▁time\n",
            "  - ate\n",
            "  - ▁has\n",
            "  - ▁people\n",
            "  - ▁see\n",
            "  - ▁pa\n",
            "  - el\n",
            "  - ▁get\n",
            "  - ▁ex\n",
            "  - ▁mean\n",
            "  - li\n",
            "  - ▁really\n",
            "  - v\n",
            "  - ▁ra\n",
            "  - ▁been\n",
            "  - ▁said\n",
            "  - '-'\n",
            "  - la\n",
            "  - ge\n",
            "  - ▁how\n",
            "  - ▁po\n",
            "  - ir\n",
            "  - ▁mo\n",
            "  - ▁who\n",
            "  - ▁because\n",
            "  - ▁co\n",
            "  - ▁other\n",
            "  - ▁f\n",
            "  - id\n",
            "  - ol\n",
            "  - ▁un\n",
            "  - ▁now\n",
            "  - ▁work\n",
            "  - ist\n",
            "  - us\n",
            "  - ▁your\n",
            "  - ▁them\n",
            "  - ver\n",
            "  - as\n",
            "  - ne\n",
            "  - ▁ca\n",
            "  - lo\n",
            "  - ▁fa\n",
            "  - ▁him\n",
            "  - ng\n",
            "  - ▁good\n",
            "  - ▁could\n",
            "  - ▁pro\n",
            "  - ive\n",
            "  - ▁con\n",
            "  - de\n",
            "  - un\n",
            "  - age\n",
            "  - ▁ma\n",
            "  - '?'\n",
            "  - at\n",
            "  - ▁ro\n",
            "  - ▁ba\n",
            "  - ▁then\n",
            "  - ▁com\n",
            "  - est\n",
            "  - vi\n",
            "  - ▁dis\n",
            "  - ies\n",
            "  - ance\n",
            "  - ▁su\n",
            "  - ▁even\n",
            "  - ▁any\n",
            "  - ut\n",
            "  - ad\n",
            "  - ul\n",
            "  - ▁se\n",
            "  - ▁two\n",
            "  - ▁bu\n",
            "  - ▁lo\n",
            "  - ▁say\n",
            "  - ▁la\n",
            "  - ▁fi\n",
            "  - is\n",
            "  - ▁li\n",
            "  - ▁over\n",
            "  - ▁new\n",
            "  - ▁man\n",
            "  - ▁sp\n",
            "  - ity\n",
            "  - ▁did\n",
            "  - ▁bo\n",
            "  - ▁very\n",
            "  - x\n",
            "  - end\n",
            "  - ▁which\n",
            "  - ▁our\n",
            "  - ▁after\n",
            "  - ▁o\n",
            "  - ke\n",
            "  - ▁p\n",
            "  - im\n",
            "  - ▁want\n",
            "  - ▁ha\n",
            "  - ▁v\n",
            "  - z\n",
            "  - ▁where\n",
            "  - ard\n",
            "  - um\n",
            "  - ▁into\n",
            "  - ru\n",
            "  - ▁di\n",
            "  - ▁lot\n",
            "  - ▁dr\n",
            "  - mp\n",
            "  - ▁day\n",
            "  - ated\n",
            "  - ci\n",
            "  - ▁these\n",
            "  - ▁than\n",
            "  - ▁take\n",
            "  - ▁kind\n",
            "  - ▁got\n",
            "  - ight\n",
            "  - ▁make\n",
            "  - ence\n",
            "  - ▁pre\n",
            "  - ▁going\n",
            "  - ish\n",
            "  - ▁k\n",
            "  - able\n",
            "  - ▁look\n",
            "  - ti\n",
            "  - per\n",
            "  - ▁here\n",
            "  - ▁en\n",
            "  - ▁ah\n",
            "  - ry\n",
            "  - ▁too\n",
            "  - ▁part\n",
            "  - ant\n",
            "  - one\n",
            "  - ▁ho\n",
            "  - ▁much\n",
            "  - ▁way\n",
            "  - ▁sa\n",
            "  - ▁something\n",
            "  - mo\n",
            "  - ▁us\n",
            "  - ▁th\n",
            "  - ▁mhm\n",
            "  - ▁mi\n",
            "  - ▁off\n",
            "  - pe\n",
            "  - ▁back\n",
            "  - les\n",
            "  - ▁cr\n",
            "  - ▁ri\n",
            "  - ▁fe\n",
            "  - und\n",
            "  - ▁fl\n",
            "  - port\n",
            "  - ▁school\n",
            "  - ▁ch\n",
            "  - ▁should\n",
            "  - ▁first\n",
            "  - ▁only\n",
            "  - ▁le\n",
            "  - ot\n",
            "  - tion\n",
            "  - ▁little\n",
            "  - ▁da\n",
            "  - ▁hu\n",
            "  - ▁d\n",
            "  - me\n",
            "  - ta\n",
            "  - ▁down\n",
            "  - ▁okay\n",
            "  - ▁come\n",
            "  - ain\n",
            "  - ff\n",
            "  - ▁car\n",
            "  - co\n",
            "  - ▁need\n",
            "  - ture\n",
            "  - ▁many\n",
            "  - ▁things\n",
            "  - ▁ta\n",
            "  - qu\n",
            "  - man\n",
            "  - ty\n",
            "  - iv\n",
            "  - ▁year\n",
            "  - he\n",
            "  - ▁thing\n",
            "  - ho\n",
            "  - ▁singapore\n",
            "  - po\n",
            "  - ▁vi\n",
            "  - ▁sc\n",
            "  - ▁still\n",
            "  - der\n",
            "  - ▁hi\n",
            "  - ▁never\n",
            "  - ▁qu\n",
            "  - ia\n",
            "  - ▁fr\n",
            "  - ▁min\n",
            "  - ▁most\n",
            "  - om\n",
            "  - ful\n",
            "  - ▁bi\n",
            "  - ▁long\n",
            "  - ig\n",
            "  - ▁years\n",
            "  - ous\n",
            "  - ▁three\n",
            "  - ▁play\n",
            "  - ▁before\n",
            "  - ▁pi\n",
            "  - ical\n",
            "  - ▁those\n",
            "  - ▁comp\n",
            "  - huh\n",
            "  - ▁live\n",
            "  - tor\n",
            "  - ise\n",
            "  - ▁old\n",
            "  - am\n",
            "  - rr\n",
            "  - ▁sta\n",
            "  - ▁n\n",
            "  - ick\n",
            "  - di\n",
            "  - ma\n",
            "  - ary\n",
            "  - ction\n",
            "  - ▁friend\n",
            "  - ition\n",
            "  - ▁gu\n",
            "  - ▁through\n",
            "  - pp\n",
            "  - for\n",
            "  - ie\n",
            "  - ious\n",
            "  - ▁sh\n",
            "  - ▁home\n",
            "  - lu\n",
            "  - ▁high\n",
            "  - ian\n",
            "  - cu\n",
            "  - ▁help\n",
            "  - ▁give\n",
            "  - ▁talk\n",
            "  - ▁sha\n",
            "  - ▁such\n",
            "  - ▁didn\n",
            "  - em\n",
            "  - ▁may\n",
            "  - ▁ga\n",
            "  - ▁'\n",
            "  - ▁gra\n",
            "  - ▁guess\n",
            "  - ▁every\n",
            "  - ▁app\n",
            "  - tic\n",
            "  - ▁tra\n",
            "  - ▁\"\n",
            "  - op\n",
            "  - ▁made\n",
            "  - '\"'\n",
            "  - ▁op\n",
            "  - ▁own\n",
            "  - ▁mar\n",
            "  - 'no'\n",
            "  - ▁ph\n",
            "  - ▁life\n",
            "  - ▁y\n",
            "  - ak\n",
            "  - ine\n",
            "  - ▁pu\n",
            "  - ▁place\n",
            "  - ▁always\n",
            "  - ▁start\n",
            "  - ▁jo\n",
            "  - ▁pe\n",
            "  - ▁let\n",
            "  - ▁name\n",
            "  - ni\n",
            "  - ▁same\n",
            "  - ▁last\n",
            "  - ▁cl\n",
            "  - ph\n",
            "  - ▁both\n",
            "  - ▁pri\n",
            "  - ities\n",
            "  - ▁another\n",
            "  - and\n",
            "  - ▁al\n",
            "  - ▁boy\n",
            "  - ving\n",
            "  - ▁actually\n",
            "  - ▁person\n",
            "  - ▁went\n",
            "  - ▁yes\n",
            "  - ca\n",
            "  - ally\n",
            "  - ▁h\n",
            "  - ▁great\n",
            "  - ▁thought\n",
            "  - ▁used\n",
            "  - act\n",
            "  - ▁feel\n",
            "  - ward\n",
            "  - ▁different\n",
            "  - ▁cons\n",
            "  - ▁show\n",
            "  - ▁watch\n",
            "  - ▁being\n",
            "  - ▁money\n",
            "  - ay\n",
            "  - ▁try\n",
            "  - ▁why\n",
            "  - ▁big\n",
            "  - ens\n",
            "  - ▁cha\n",
            "  - ▁find\n",
            "  - ▁hand\n",
            "  - ▁real\n",
            "  - ▁four\n",
            "  - ial\n",
            "  - ▁ne\n",
            "  - ▁che\n",
            "  - ▁read\n",
            "  - ▁five\n",
            "  - ▁family\n",
            "  - ag\n",
            "  - ▁change\n",
            "  - ▁add\n",
            "  - ha\n",
            "  - ▁put\n",
            "  - par\n",
            "  - lic\n",
            "  - side\n",
            "  - ▁came\n",
            "  - ▁under\n",
            "  - ness\n",
            "  - ▁per\n",
            "  - j\n",
            "  - ▁around\n",
            "  - ▁end\n",
            "  - ▁house\n",
            "  - if\n",
            "  - ▁while\n",
            "  - vo\n",
            "  - ▁act\n",
            "  - ▁happen\n",
            "  - ▁plan\n",
            "  - mit\n",
            "  - ▁far\n",
            "  - ▁tri\n",
            "  - ▁ten\n",
            "  - ▁du\n",
            "  - ▁win\n",
            "  - ▁tea\n",
            "  - ze\n",
            "  - ▁better\n",
            "  - ▁sure\n",
            "  - ▁mu\n",
            "  - ▁use\n",
            "  - ▁anything\n",
            "  - ▁love\n",
            "  - ▁world\n",
            "  - ▁hard\n",
            "  - ure\n",
            "  - ▁does\n",
            "  - ▁war\n",
            "  - ▁stuff\n",
            "  - ▁ja\n",
            "  - ▁must\n",
            "  - min\n",
            "  - gg\n",
            "  - ▁ru\n",
            "  - ▁care\n",
            "  - ▁tell\n",
            "  - ▁pl\n",
            "  - ▁doing\n",
            "  - ▁probably\n",
            "  - ▁found\n",
            "  - ative\n",
            "  - ▁point\n",
            "  - ach\n",
            "  - ▁ju\n",
            "  - ip\n",
            "  - ▁again\n",
            "  - ▁interest\n",
            "  - ▁state\n",
            "  - ▁week\n",
            "  - na\n",
            "  - ▁might\n",
            "  - ▁pretty\n",
            "  - ▁ki\n",
            "  - ▁fo\n",
            "  - ber\n",
            "  - ▁am\n",
            "  - line\n",
            "  - led\n",
            "  - ▁six\n",
            "  - ▁acc\n",
            "  - ▁bri\n",
            "  - ▁call\n",
            "  - ▁sw\n",
            "  - ▁each\n",
            "  - ▁business\n",
            "  - ▁keep\n",
            "  - ▁away\n",
            "  - cause\n",
            "  - ▁pass\n",
            "  - ▁va\n",
            "  - ▁children\n",
            "  - ▁pay\n",
            "  - ▁count\n",
            "  - ▁public\n",
            "  - ▁everything\n",
            "  - land\n",
            "  - ▁though\n",
            "  - ▁men\n",
            "  - bo\n",
            "  - ▁young\n",
            "  - ▁na\n",
            "  - ▁move\n",
            "  - ough\n",
            "  - ating\n",
            "  - com\n",
            "  - ▁month\n",
            "  - ton\n",
            "  - ▁close\n",
            "  - ▁few\n",
            "  - '!'\n",
            "  - ▁maybe\n",
            "  - ▁imp\n",
            "  - son\n",
            "  - ▁grow\n",
            "  - ▁u\n",
            "  - ▁turn\n",
            "  - ible\n",
            "  - ▁em\n",
            "  - ▁air\n",
            "  - ▁ever\n",
            "  - our\n",
            "  - ▁sea\n",
            "  - ▁fun\n",
            "  - ▁government\n",
            "  - ▁miss\n",
            "  - ▁done\n",
            "  - ▁next\n",
            "  - ▁kids\n",
            "  - ▁cor\n",
            "  - ▁set\n",
            "  - ▁run\n",
            "  - way\n",
            "  - ▁wa\n",
            "  - ▁getting\n",
            "  - ▁eight\n",
            "  - ▁open\n",
            "  - ▁job\n",
            "  - ▁problem\n",
            "  - ook\n",
            "  - ▁night\n",
            "  - ▁learn\n",
            "  - ▁book\n",
            "  - ual\n",
            "  - ▁ti\n",
            "  - ▁best\n",
            "  - cept\n",
            "  - ▁during\n",
            "  - ▁small\n",
            "  - ex\n",
            "  - ▁without\n",
            "  - ▁water\n",
            "  - ▁trans\n",
            "  - ▁course\n",
            "  - ▁once\n",
            "  - ▁sit\n",
            "  - ▁area\n",
            "  - ▁country\n",
            "  - ▁mister\n",
            "  - ▁nothing\n",
            "  - ▁whole\n",
            "  - ▁believe\n",
            "  - ▁service\n",
            "  - ▁took\n",
            "  - ▁face\n",
            "  - ▁bad\n",
            "  - ▁later\n",
            "  - ▁head\n",
            "  - ▁called\n",
            "  - ▁seven\n",
            "  - ▁art\n",
            "  - ▁since\n",
            "  - ▁er\n",
            "  - ▁fact\n",
            "  - ▁city\n",
            "  - ▁market\n",
            "  - ▁hour\n",
            "  - ▁continue\n",
            "  - ship\n",
            "  - ▁invest\n",
            "  - ▁exactly\n",
            "  - ▁large\n",
            "  - ▁true\n",
            "  - ▁nine\n",
            "  - ▁sub\n",
            "  - ▁having\n",
            "  - ▁game\n",
            "  - va\n",
            "  - ▁lu\n",
            "  - ▁conf\n",
            "  - ▁case\n",
            "  - ▁doesn\n",
            "  - ▁certain\n",
            "  - ▁wi\n",
            "  - ▁law\n",
            "  - ▁else\n",
            "  - fi\n",
            "  - ▁left\n",
            "  - ▁enough\n",
            "  - ▁second\n",
            "  - ▁gonna\n",
            "  - ▁food\n",
            "  - ▁hope\n",
            "  - ▁saw\n",
            "  - ▁between\n",
            "  - ▁je\n",
            "  - bi\n",
            "  - ▁girl\n",
            "  - ▁company\n",
            "  - ▁able\n",
            "  - ▁expect\n",
            "  - ▁told\n",
            "  - ▁stand\n",
            "  - ▁group\n",
            "  - ▁main\n",
            "  - ▁walk\n",
            "  - ▁cause\n",
            "  - ▁however\n",
            "  - ▁number\n",
            "  - ▁follow\n",
            "  - ▁near\n",
            "  - ▁yet\n",
            "  - ▁sometimes\n",
            "  - ▁train\n",
            "  - ▁lead\n",
            "  - ▁system\n",
            "  - ▁remain\n",
            "  - ▁develop\n",
            "  - gra\n",
            "  - ▁word\n",
            "  - ▁exc\n",
            "  - ▁together\n",
            "  - ▁consider\n",
            "  - ▁town\n",
            "  - ▁less\n",
            "  - ator\n",
            "  - ▁important\n",
            "  - ▁remember\n",
            "  - ▁free\n",
            "  - ▁quite\n",
            "  - ▁understand\n",
            "  - ▁bra\n",
            "  - ▁support\n",
            "  - ▁idea\n",
            "  - ▁stop\n",
            "  - ▁reason\n",
            "  - ▁nice\n",
            "  - ▁mm\n",
            "  - ▁agree\n",
            "  - ▁low\n",
            "  - ▁against\n",
            "  - ▁issue\n",
            "  - ▁become\n",
            "  - ▁today\n",
            "  - ▁side\n",
            "  - ▁student\n",
            "  - ▁matter\n",
            "  - ▁question\n",
            "  - ▁mother\n",
            "  - ▁father\n",
            "  - ▁hundred\n",
            "  - ▁sort\n",
            "  - ▁eat\n",
            "  - ▁already\n",
            "  - ▁rest\n",
            "  - ▁line\n",
            "  - ▁asked\n",
            "  - ▁include\n",
            "  - ▁upon\n",
            "  - ▁office\n",
            "  - ▁won\n",
            "  - ▁class\n",
            "  - ▁wait\n",
            "  - ▁twenty\n",
            "  - ▁half\n",
            "  - ▁light\n",
            "  - ▁price\n",
            "  - ▁almost\n",
            "  - ash\n",
            "  - ▁child\n",
            "  - ▁sign\n",
            "  - ▁least\n",
            "  - ▁several\n",
            "  - press\n",
            "  - ▁either\n",
            "  - ▁minute\n",
            "  - ▁himself\n",
            "  - ▁parents\n",
            "  - ▁room\n",
            "  - ▁whatever\n",
            "  - ▁general\n",
            "  - ▁cost\n",
            "  - ▁among\n",
            "  - ▁direct\n",
            "  - ▁computer\n",
            "  - ▁appear\n",
            "  - ▁meet\n",
            "  - ▁ski\n",
            "  - ▁return\n",
            "  - ▁couple\n",
            "  - ▁product\n",
            "  - ▁suppose\n",
            "  - ▁definitely\n",
            "  - ▁america\n",
            "  - ▁term\n",
            "  - ▁usually\n",
            "  - ▁strong\n",
            "  - ▁current\n",
            "  - ▁arm\n",
            "  - ▁speak\n",
            "  - ▁local\n",
            "  - ▁south\n",
            "  - ▁experience\n",
            "  - ▁full\n",
            "  - ▁north\n",
            "  - ▁elect\n",
            "  - ▁leave\n",
            "  - ▁provide\n",
            "  - qui\n",
            "  - ▁power\n",
            "  - ▁movie\n",
            "  - ▁everyone\n",
            "  - ▁making\n",
            "  - ▁member\n",
            "  - ▁woman\n",
            "  - ▁somebody\n",
            "  - ▁wonder\n",
            "  - ▁short\n",
            "  - ▁health\n",
            "  - ▁police\n",
            "  - ▁bank\n",
            "  - ▁until\n",
            "  - ▁companies\n",
            "  - ▁everybody\n",
            "  - ▁knew\n",
            "  - ▁program\n",
            "  - ▁music\n",
            "  - ▁york\n",
            "  - ▁land\n",
            "  - ▁doctor\n",
            "  - ▁answer\n",
            "  - ▁building\n",
            "  - ▁employ\n",
            "  - ▁travel\n",
            "  - ▁major\n",
            "  - ▁seems\n",
            "  - ▁safe\n",
            "  - gue\n",
            "  - ▁college\n",
            "  - ▁along\n",
            "  - ▁clear\n",
            "  - ▁especially\n",
            "  - ▁umhu\n",
            "  - ▁result\n",
            "  - ▁type\n",
            "  - ▁court\n",
            "  - ▁black\n",
            "  - ▁hold\n",
            "  - ▁myself\n",
            "  - ▁education\n",
            "  - ▁social\n",
            "  - ▁enjoy\n",
            "  - ▁became\n",
            "  - ▁whether\n",
            "  - ▁morning\n",
            "  - ▁difficult\n",
            "  - ▁shi\n",
            "  - ▁felt\n",
            "  - ▁husband\n",
            "  - ▁white\n",
            "  - ▁taking\n",
            "  - ▁million\n",
            "  - ▁require\n",
            "  - ▁early\n",
            "  - ency\n",
            "  - ▁visit\n",
            "  - ▁level\n",
            "  - ▁brother\n",
            "  - ▁married\n",
            "  - ▁further\n",
            "  - ▁affect\n",
            "  - ▁serve\n",
            "  - ▁present\n",
            "  - ▁park\n",
            "  - ▁effect\n",
            "  - ▁wife\n",
            "  - ▁teacher\n",
            "  - ▁cannot\n",
            "  - ▁community\n",
            "  - ▁street\n",
            "  - ▁period\n",
            "  - ▁national\n",
            "  - ▁view\n",
            "  - ▁future\n",
            "  - ▁daughter\n",
            "  - ▁situation\n",
            "  - ▁grand\n",
            "  - ▁success\n",
            "  - ▁perform\n",
            "  - ▁concern\n",
            "  - ▁complete\n",
            "  - ▁example\n",
            "  - ized\n",
            "  - ▁thousand\n",
            "  - ▁increase\n",
            "  - ▁began\n",
            "  - ▁final\n",
            "  - ▁east\n",
            "  - ▁sense\n",
            "  - ▁charge\n",
            "  - ▁record\n",
            "  - ▁born\n",
            "  - ▁instead\n",
            "  - ▁receive\n",
            "  - ▁women\n",
            "  - ▁across\n",
            "  - ▁information\n",
            "  - ▁although\n",
            "  - ▁process\n",
            "  - ▁condition\n",
            "  - ▁security\n",
            "  - ▁treat\n",
            "  - ▁funny\n",
            "  - ▁custom\n",
            "  - ▁cold\n",
            "  - ▁behind\n",
            "  - ified\n",
            "  - ▁ground\n",
            "  - cycl\n",
            "  - ▁depend\n",
            "  - ▁themselves\n",
            "  - ▁design\n",
            "  - ▁slow\n",
            "  - ▁third\n",
            "  - ▁smoke\n",
            "  - ▁wrong\n",
            "  - ▁project\n",
            "  - ▁space\n",
            "  - ▁drink\n",
            "  - ▁particular\n",
            "  - ▁listen\n",
            "  - ▁thirty\n",
            "  - ▁special\n",
            "  - ability\n",
            "  - ▁improve\n",
            "  - ▁attack\n",
            "  - ▁happy\n",
            "  - ▁strange\n",
            "  - ▁english\n",
            "  - ▁value\n",
            "  - ▁brought\n",
            "  - ▁private\n",
            "  - ▁account\n",
            "  - ▁china\n",
            "  - ▁spoke\n",
            "  - ▁foreign\n",
            "  - ▁possible\n",
            "  - ▁author\n",
            "  - ▁circ\n",
            "  - ▁voice\n",
            "  - ▁figure\n",
            "  - ▁control\n",
            "  - ▁according\n",
            "  - ▁green\n",
            "  - ▁university\n",
            "  - ▁language\n",
            "  - ▁please\n",
            "  - ▁animal\n",
            "  - ▁church\n",
            "  - ▁society\n",
            "  - ▁dream\n",
            "  - ’\n",
            "  - q\n",
            "  - ':'\n",
            "  - ;\n",
            "  - —\n",
            "  - ‘\n",
            "  - ”\n",
            "  - _\n",
            "  - '3'\n",
            "  - '8'\n",
            "  - <\n",
            "  - '>'\n",
            "  - '1'\n",
            "  - –\n",
            "  - '7'\n",
            "  - (\n",
            "  - )\n",
            "  - '0'\n",
            "  - '2'\n",
            "  - '4'\n",
            "  - +\n",
            "  - '&'\n",
            "  - '5'\n",
            "  - '9'\n",
            "  - ü\n",
            "  - é\n",
            "  - /\n",
            "  - á\n",
            "  - ó\n",
            "  - ō\n",
            "  - ú\n",
            "  - ']'\n",
            "  - â\n",
            "  - í\n",
            "  - ã\n",
            "  - ð\n",
            "  - ā\n",
            "  - ć\n",
            "  - č\n",
            "  - š\n",
            "  - è\n",
            "  - ë\n",
            "  - '`'\n",
            "  - ç\n",
            "  - ū\n",
            "  - ạ\n",
            "  - ø\n",
            "  - '='\n",
            "  - à\n",
            "  - ł\n",
            "  - α\n",
            "  - ô\n",
            "  - к\n",
            "  - '}'\n",
            "  - å\n",
            "  - ă\n",
            "  - и\n",
            "  - ī\n",
            "  - π\n",
            "  - œ\n",
            "  - \\\n",
            "  - '['\n",
            "  - ñ\n",
            "  - ß\n",
            "  - ö\n",
            "  - ä\n",
            "  - '6'\n",
            "  - з\n",
            "  - н\n",
            "  - û\n",
            "  - '%'\n",
            "  - '{'\n",
            "  - ¡\n",
            "  - æ\n",
            "  - ê\n",
            "  - þ\n",
            "  - ę\n",
            "  - ě\n",
            "  - ğ\n",
            "  - ń\n",
            "  - ő\n",
            "  - ř\n",
            "  - ž\n",
            "  - ʻ\n",
            "  - в\n",
            "  - е\n",
            "  - й\n",
            "  - л\n",
            "  - ь\n",
            "  - χ\n",
            "  - “\n",
            "optim:\n",
            "  name: novograd\n",
            "  lr: 0.05\n",
            "  betas:\n",
            "  - 0.8\n",
            "  - 0.25\n",
            "  weight_decay: 0.001\n",
            "  sched:\n",
            "    name: CosineAnnealing\n",
            "    warmup_steps: 1000\n",
            "    min_lr: 1.0e-06\n",
            "    max_steps: 100\n",
            "target: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE\n",
            "nemo_version: 2.0.0\n",
            "decoding:\n",
            "  strategy: greedy_batch\n",
            "  preserve_alignments: null\n",
            "  compute_timestamps: null\n",
            "  word_seperator: ' '\n",
            "  ctc_timestamp_type: all\n",
            "  batch_dim_index: 0\n",
            "  greedy:\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    preserve_frame_confidence: false\n",
            "    confidence_method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  beam:\n",
            "    beam_size: 4\n",
            "    search_type: default\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    return_best_hypothesis: true\n",
            "    beam_alpha: 1.0\n",
            "    beam_beta: 0.0\n",
            "    kenlm_path: null\n",
            "    flashlight_cfg:\n",
            "      lexicon_path: null\n",
            "      boost_path: null\n",
            "      beam_size_token: 16\n",
            "      beam_threshold: 20.0\n",
            "      unk_weight: -.inf\n",
            "      sil_weight: 0.0\n",
            "    pyctcdecode_cfg:\n",
            "      beam_prune_logp: -10.0\n",
            "      token_min_logp: -5.0\n",
            "      prune_history: false\n",
            "      hotwords: null\n",
            "      hotword_weight: 10.0\n",
            "  wfst:\n",
            "    beam_size: 4\n",
            "    search_type: riva\n",
            "    return_best_hypothesis: true\n",
            "    preserve_alignments: false\n",
            "    compute_timestamps: false\n",
            "    decoding_mode: nbest\n",
            "    open_vocabulary_decoding: false\n",
            "    beam_width: 10.0\n",
            "    lm_weight: 1.0\n",
            "    device: cuda\n",
            "    arpa_lm_path: null\n",
            "    wfst_lm_path: null\n",
            "    riva_decoding_cfg: {}\n",
            "    k2_decoding_cfg:\n",
            "      search_beam: 20.0\n",
            "      output_beam: 10.0\n",
            "      min_active_states: 30\n",
            "      max_active_states: 10000\n",
            "  confidence_cfg:\n",
            "    preserve_frame_confidence: false\n",
            "    preserve_token_confidence: false\n",
            "    preserve_word_confidence: false\n",
            "    exclude_blank: true\n",
            "    aggregation: min\n",
            "    tdt_include_duration: false\n",
            "    method_cfg:\n",
            "      name: entropy\n",
            "      entropy_type: tsallis\n",
            "      alpha: 0.33\n",
            "      entropy_norm: exp\n",
            "      temperature: DEPRECATED\n",
            "  temperature: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Note that the preprocessor + optimizer config have been preserved after the changes we made !\n",
        "print(OmegaConf.to_yaml(temp_cn.cfg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI3RxwpcV-UF"
      },
      "source": [
        "Nota: el archivo .nemo es un simple .tar.gz con el punto de control, la configuración y, potencialmente, otros artefactos como las configuraciones del tokenizador que está utilizando el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFBAGcaDWLiu",
        "outputId": "495b9bb0-e966-4ffd-ef90-6dcfd694ab75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./\n",
            "./model_config.yaml\n",
            "./model_weights.ckpt\n",
            "./tokenizer.model\n",
            "./vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!cp citrinet_512.nemo citrinet_512.tar.gz\n",
        "!tar -xvf citrinet_512.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkau4Q9jZo1l"
      },
      "source": [
        "### Extrayendo puntos de control de PyTorch desde archivos tar de NeMo (Nivel de Modelo)\n",
        "-----------\n",
        "\n",
        "Si bien el archivo .nemo tar es una excelente manera de tener un modelo portátil, a veces es necesario que los investigadores tengan acceso al formato de guardado básico de PyTorch. NeMo tiene como objetivo ser completamente compatible con PyTorch y, por lo tanto, ofrece un método simple para extraer solo el punto de control de PyTorch desde el archivo .nemo tar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qccPANeycCoq"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4zswOKHar9q",
        "outputId": "b0c92b7c-1d30-4910-ac85-95861aa42afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:20:10 save_restore_connector:346] Checkpoints from citrinet_512.nemo were successfully extracted into /content/pt_ckpt.\n",
            "model_weights.ckpt\n"
          ]
        }
      ],
      "source": [
        "state_dict = temp_cn.extract_state_dict_from('citrinet_512.nemo', save_dir='./pt_ckpt/')\n",
        "!ls ./pt_ckpt/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACB-0dfnbFG3"
      },
      "source": [
        "Como podemos ver a continuación, ahora hay un único punto de control básico de PyTorch disponible dentro del directorio `pt_ckpt`, que podemos usar para cargar los pesos de todo el modelo como se muestra a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZAF_A0uc5bB",
        "outputId": "a38538a7-581d-467f-877a-43c26e9957d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:20:15 nemo_logging:349] <ipython-input-28-85377080a5db>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      temp_cn.load_state_dict(torch.load('./pt_ckpt/model_weights.ckpt'))\n",
            "    \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp_cn.load_state_dict(torch.load('./pt_ckpt/model_weights.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkq6EM99cS6y"
      },
      "source": [
        "### Extrayendo puntos de control de PyTorch desde archivos tar de NeMo (Nivel de Módulo)\n",
        "----------\n",
        "\n",
        "Si bien el método anterior es excepcional para extraer el punto de control de todo el modelo, a veces puede ser necesario cargar y guardar los módulos individuales que componen el Modelo.\n",
        "\n",
        "El mismo método de extracción ofrece una opción para extraer los puntos de control de nivel de modelo individual en sus archivos individuales, para que los usuarios tengan acceso a puntos de control por módulo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW6wve2zbT9D",
        "outputId": "6c1ccccd-0f04-4350-849a-d7c818ab66e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-24 08:20:20 save_restore_connector:346] Checkpoints from citrinet_512.nemo were successfully extracted into /content/pt_module_ckpt.\n",
            "decoder.ckpt  encoder.ckpt  preprocessor.ckpt\n"
          ]
        }
      ],
      "source": [
        "state_dict = temp_cn.extract_state_dict_from('citrinet_512.nemo', save_dir='./pt_module_ckpt/', split_by_module=True)\n",
        "!ls ./pt_module_ckpt/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtV5vpb5d1ni"
      },
      "source": [
        "¡Ahora, podemos cargar y asignar los pesos de los módulos individuales del modelo Citrinet mencionado anteriormente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVHylSKFdywn",
        "outputId": "87f643b4-8115-4b29-b10f-5cc1856924c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-24 08:20:21 nemo_logging:349] <ipython-input-30-16927104504f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      temp_cn.preprocessor.load_state_dict(torch.load('./pt_module_ckpt/preprocessor.ckpt'))\n",
            "    \n",
            "[NeMo W 2024-12-24 08:20:21 nemo_logging:349] <ipython-input-30-16927104504f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      temp_cn.encoder.load_state_dict(torch.load('./pt_module_ckpt/encoder.ckpt'))\n",
            "    \n",
            "[NeMo W 2024-12-24 08:20:22 nemo_logging:349] <ipython-input-30-16927104504f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      temp_cn.decoder.load_state_dict(torch.load('./pt_module_ckpt/decoder.ckpt'))\n",
            "    \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp_cn.preprocessor.load_state_dict(torch.load('./pt_module_ckpt/preprocessor.ckpt'))\n",
        "temp_cn.encoder.load_state_dict(torch.load('./pt_module_ckpt/encoder.ckpt'))\n",
        "temp_cn.decoder.load_state_dict(torch.load('./pt_module_ckpt/decoder.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88vOGV7VYcuu"
      },
      "source": [
        "# NeMo con Hydra\n",
        "\n",
        "[Hydra](https://hydra.cc/docs/intro/) se utiliza en todo NeMo como una forma de habilitar la creación rápida de prototipos utilizando archivos de configuración predefinidos. Hydra y OmegaConf ofrecen una gran compatibilidad entre sí, y a continuación mostramos algunos consejos generales útiles para mejorar la productividad con Hydra al usar NeMo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfY6Ha3qYcxG"
      },
      "source": [
        "## Ayuda de Hydra\n",
        "--------\n",
        "\n",
        "Dado que nuestros scripts están escritos con Hydra en mente, es posible que notes que al usar `python <script.py> --help` te devuelve una configuración en lugar del formato de ayuda habitual de argparse.\n",
        "\n",
        "Usando `--help` puedes ver la configuración predeterminada adjunta al script; cada script de NeMo tiene al menos un archivo de configuración predeterminado adjunto. Esto te da una guía sobre cómo puedes modificar valores para un experimento.\n",
        "\n",
        "Hydra también tiene una bandera especial `--hydra-help`, que te ofrecerá más ayuda con respecto a Hydra en sí misma tal como está configurada en el script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEsZlnfaYc3X"
      },
      "source": [
        "## Cambiando rutas y archivos de configuración\n",
        "---------\n",
        "\n",
        "Aunque todos los modelos de NeMo vienen con al menos 1 archivo de configuración predeterminado, uno podría querer cambiar las configuraciones sin cambiar el código. Esto se logra fácilmente con los siguientes comandos:\n",
        "\n",
        "- `--config-path`: Ruta al directorio que contiene los archivos de configuración.\n",
        "- `--config-name`: Nombre del archivo de configuración que deseamos cargar.\n",
        "\n",
        "Tenga en cuenta que estos dos argumentos deben estar al principio de su declaración de ejecución, antes de proporcionar cualquier anulación de línea de comandos a su archivo de configuración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyNHlArpYc9A"
      },
      "source": [
        "## Sobrescribir la configuración desde la línea de comandos\n",
        "----------\n",
        "\n",
        "Hydra permite a los usuarios proporcionar sobrescrituras de línea de comandos a cualquier parte de la configuración. Hay tres casos a considerar:\n",
        "\n",
        " - Sobrescribir un valor existente en la configuración\n",
        " - Agregar un nuevo valor en la configuración\n",
        " - Eliminar un valor antiguo en la configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96CKbvn6Yc7f"
      },
      "source": [
        "### Sobreescribiendo los valores existentes en config\n",
        "\n",
        "Tomemos el caso en el que queremos cambiar el optimizador de `novograd` a `adam`. Cambiemos también los valores beta a los valores predeterminados de adam.\n",
        "\n",
        "Las sobrescrituras de Hydra se basan en la sintaxis de `.` - cada `.` representa un nivel en la propia configuración.\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.optim.name=\"adam\" \\\n",
        "  model.optim.betas=[0.9,0.999]\n",
        "```\n",
        "\n",
        "Cabe señalar que, si se pasan listas, no puede haber espacios entre los elementos.\n",
        "\n",
        "------\n",
        "\n",
        "También podemos admitir múltiples conjuntos de datos de validación con la sintaxis de lista anterior, pero depende del soporte a nivel de modelo.\n",
        "\n",
        "Para la colección ASR, la siguiente sintaxis es ampliamente compatible en los modelos ASR, ASR-BPE y de clasificación. Tomemos un ejemplo de un modelo que se está entrenando en LibriSpeech -\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.validation_ds.manifest_filepath=[\"path to dev clean\",\"path to dev other\"] \\\n",
        "  model.test_ds.manifest_filepath=[\"path to test clean\",\"path to test other\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj7oMkepYc17"
      },
      "source": [
        "### Agregar nuevos valores en la configuración\n",
        "----------\n",
        "\n",
        "Hydra nos permite inyectar parámetros adicionales dentro de la configuración usando la sintaxis `+`.\n",
        "\n",
        "Tomemos un ejemplo de agregar la corrección `amsgrad` para el optimizador `novograd` mencionado anteriormente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23327hsYc0Z"
      },
      "source": [
        "### Eliminar valor antiguo en la configuración\n",
        "---------\n",
        "\n",
        "Hydra nos permite eliminar parámetros dentro de la configuración usando la sintaxis `~`.\n",
        "\n",
        "Tomemos un ejemplo de eliminar `weight_decay` dentro del optimizador Novograd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VSWIbzjYzDi"
      },
      "source": [
        "## Fijando un valor a `None` desde la línea de comandos\n",
        "\n",
        "Podemos optar por deshabilitar una característica estableciendo el valor en `None`.\n",
        "\n",
        "Podemos lograr esto utilizando la palabra clave `null` en la línea de comandos.\n",
        "\n",
        "Tomemos un ejemplo de deshabilitar el cargador de datos de validación dentro de la configuración de un modelo ASR -\n",
        "\n",
        "\n",
        "```sh\n",
        "$ python <script>.py \\\n",
        "  --config-path=\"dir to config\" \\\n",
        "  --config-name=\"name of config\" \\\n",
        "  model.test_ds.manifest_filepath=null\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah8rgrvvsw5R"
      },
      "source": [
        "# Ejemplos de NeMo\n",
        "\n",
        "NeMo admite varios modelos preconstruidos para tareas de ASR, NLP y TTS. Un ejemplo que vemos en este cuaderno es el modelo ASR para Conversión de Voz a Texto, utilizando el modelo Citrinet.\n",
        "\n",
        "El repositorio de NeMo tiene un directorio dedicado `examples` con scripts para entrenar y evaluar modelos para varias tareas, que van desde ASR conversión de voz a texto, NLP respuesta a preguntas y TTS conversión de texto a voz utilizando modelos como `FastPitch` y `HiFiGAN`.\n",
        "\n",
        "NeMo constantemente agrega nuevos modelos y nuevas tareas a estos ejemplos, de manera que estos ejemplos sirven como base para entrenar y evaluar modelos desde cero con los archivos de configuración proporcionados.\n",
        "\n",
        "El directorio de Ejemplos de NeMo se puede encontrar aquí - https://github.com/NVIDIA/NeMo/tree/main/examples"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
