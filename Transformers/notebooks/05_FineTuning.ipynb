{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning de un modelo preentrenado\n",
    "\n",
    "*Este tutorial es una versi칩n recortada y adaptada de:* https://huggingface.co/docs/transformers/v4.47.1/es/training \n",
    "\n",
    "El entrenamiento de un modelo partiendo de una inicializaci칩n aleatoria de los par치metros del mismo es una tarea muy costosa computacionalmente. Especialmente para modelos con arquitectura de tipo Transformers, donde los requerimientos computacionales pueden implicar el uso de cientos de GPUs durante semanas, e incluso m치s. Por ello, es m치s habitual en entornos modestos partir de una inicializaci칩n de los pesos procedente de un modelo preentrenado para una tarea m치s general que aquella que se quiere acometer. \n",
    "\n",
    "Este proceso, que tiene unos requerimientos computacionales mucho m치s modestos recibe el nombre de **fine-tuning**. En este caso, los pesos ya est치n inicializados para resolver una tarea m치s gen칠rica, pero se expone durante varios ciclos de entrenamiento a un *dataset* m치s peque침o que el inicial, pero dise침ado para que el modelo aprenda una tarea m치s espec칤fica. \n",
    "\n",
    "Este notebook va a demostrar c칩mo realizar dicho proceso utilizando la librer칤a Transformers. Se va a realizar siguiendo dos v칤as diferentes.\n",
    "\n",
    "La m치s sencilla, y la 칰nica que vamos a explorar en esta demostraci칩n, es utilizar la clase Trainer de la librer칤a transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparaci칩n del *dataset*\n",
    "\n",
    "El primer paso para hacer fine-tuning de un modelo es descargar un dataset (o crear el nuestro propio) y prepararlo para el entrenamiento. El notebook anterior (04) nos ense침칩 varias t칠cnicas para realizar este proceso utilizando la librer칤a transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ulc/es/dac/.cache/huggingface/datasets/parquet/yelp_review_full-66f1f8c8d1a2da02/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc57738fc0741f29298212809ae4fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2f59ddc6324c4293f22379112c80db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad77530c39f46a38389e7f628b317ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5184b924a64c4bfeb0e9b16c74967799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa81564cb744e039389039d1d7375cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb93a01e7af34b1e8a86c23a460790c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c6502f1184509b643f76f6aebc640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descargar el dataset de review de yelp\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "# Imprimir un ejemplo del dataset\n",
    "dataset[\"train\"][100]\n",
    "from transformers import AutoTokenizer\n",
    "# Descargar el tokenizador de BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# Funci칩n para tokenizar los ejemplos, se aplica truncation y padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "# Aplicar la funci칩n a todo el dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Utilizar un subconjunto del dataset reducido si no queremos entrenar con todo el dataset\n",
    "# Si lo utilizamos todo, se tarda mucho en entrenar\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con Pytorch Trainer\n",
    "\n",
    "La librer칤a Transformers proporciona una clase *Trainer* optimizada para el entrenamiento de modelos Transformer. Evita que tengamos que escribir nuestro propio bucle de entrenamiento. El API *Trainer* permite la configuraci칩n de una gama amplia de par치metros de configuraci칩n del entrenamiento y de caracter칤sticas adicionales como *logging*, *gradiente accumulation* y *mixed precision*.\n",
    "\n",
    "El primer paso es cargar tu modelo y especificar el n칰mero de etiquetas (*labels*) esperadas. En la documentaci칩n del dataset de [Yelp](https://huggingface.co/datasets/yelp_review_full#data-fields) vemos que hay 5 etiquetas.\n",
    "\n",
    "Por defecto, los pesos se cargan en precisi칩n total (*torch.float32*), sin importar si los datos estaban almacenados en una precisi칩n m치s baja, por ejemplo *half precision* (*torch.float16*). Si queremos evitar esto, y que el modelo se cargue en la precisi칩n en la que fue almacenado, tenemos que fijar el par치metro *torch_dtype=\"auto\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07be3f164a044d8390539f7643504761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperpar치metros de entrenamiento\n",
    "\n",
    "Para fijar los hiperpar치metros ajustables del entrenamiento, crear una clase llamada *TrainingArguments* que contiene todos los hiperpar치metros que puede ajustar adem치s de *flags* para activar distintas opciones de entrenamiento. En esta demostraci칩n dejaremos los valores por defecto, pero puedes experimentar a cambiar distintos par치metros y ver qu칠 efecto tienen en el entreanmiento. ([documentaci칩n TrainingArguments](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.TrainingArguments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci칩n del modelo\n",
    "\n",
    "El Trainer no eval칰a autom치ticamente el rendimiento del modelo durante el entrenamiento. Tendr치s que pasarle a Trainer una funci칩n para calcular y hacer un reporte de las m칠tricas. La biblioteca de Datasets proporciona una funci칩n de accuracy simple que puedes cargar con la funci칩n load_metric (ver [este tutorial](https://huggingface.co/docs/datasets/metrics) para m치s informaci칩n):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/ulc/es/dac/mytx/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 游뱅 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9041439410464d07b4ea0894606d44d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define la funci칩n compute en metric para calcular el accuracy de tus predicciones. Antes de pasar tus predicciones a compute, necesitas convertir las predicciones a logits (recuerda que todos los modelos de Transformers devuelven logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quieres controlar tus m칠tricas de evaluaci칩n durante el fine-tuning, especifica el par치metro eval_strategy en tus argumentos de entrenamiento para que el modelo tenga en cuenta la m칠trica de evaluaci칩n al final de cada 칠poca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Crea un objeto Trainer con tu modelo, argumentos de entrenamiento, datasets de entrenamiento y de prueba, y tu funci칩n de evaluaci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci칩n, aplica fine-tuning a tu modelo llamando train():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/ulc/es/dac/mytx/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.367411</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.034930</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.984887</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.1327731119791666, metrics={'train_runtime': 356.644, 'train_samples_per_second': 8.412, 'train_steps_per_second': 1.051, 'total_flos': 789354427392000.0, 'train_loss': 1.1327731119791666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen dos v칤as m치s complicadas para realizar el mismo proceso, pero que a cambio proporcionan m치s control sobre el mismo.\n",
    "\n",
    "- Fine-tuning con Keras: [Ejemplo](https://huggingface.co/docs/transformers/v4.47.1/es/training#fine-tuning-con-keras)\n",
    "- Fine-tuning con Pytorch Nativo: [Ejemplo](https://huggingface.co/docs/transformers/v4.47.1/es/training#fine-tune-en-pytorch-nativo)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytx",
   "language": "python",
   "name": "mytx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
