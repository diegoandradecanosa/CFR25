{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning de un modelo preentrenado\n",
    "\n",
    "*Este tutorial es una versión recortada y adaptada de:* https://huggingface.co/docs/transformers/v4.47.1/es/training \n",
    "\n",
    "El entrenamiento de un modelo partiendo de una inicialización aleatoria de los parámetros del mismo es una tarea muy costosa computacionalmente. Especialmente para modelos con arquitectura de tipo Transformers, donde los requerimientos computacionales pueden implicar el uso de cientos de GPUs durante semanas, e incluso más. Por ello, es más habitual en entornos modestos partir de una inicialización de los pesos procedente de un modelo preentrenado para una tarea más general que aquella que se quiere acometer. \n",
    "\n",
    "Este proceso, que tiene unos requerimientos computacionales mucho más modestos recibe el nombre de **fine-tuning**. En este caso, los pesos ya están inicializados para resolver una tarea más genérica, pero se expone durante varios ciclos de entrenamiento a un *dataset* más pequeño que el inicial, pero diseñado para que el modelo aprenda una tarea más específica. \n",
    "\n",
    "Este notebook va a demostrar cómo realizar dicho proceso utilizando la librería Transformers. Se va a realizar siguiendo dos vías diferentes.\n",
    "\n",
    "La más sencilla, y la única que vamos a explorar en esta demostración, es utilizar la clase Trainer de la librería transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del *dataset*\n",
    "\n",
    "El primer paso para hacer fine-tuning de un modelo es descargar un dataset (o crear el nuestro propio) y prepararlo para el entrenamiento. El notebook anterior (04) nos enseñó varias técnicas para realizar este proceso utilizando la librería transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ulc/es/dac/.cache/huggingface/datasets/parquet/yelp_review_full-66f1f8c8d1a2da02/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc57738fc0741f29298212809ae4fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2f59ddc6324c4293f22379112c80db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad77530c39f46a38389e7f628b317ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5184b924a64c4bfeb0e9b16c74967799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa81564cb744e039389039d1d7375cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb93a01e7af34b1e8a86c23a460790c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c6502f1184509b643f76f6aebc640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descargar el dataset de review de yelp\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "# Imprimir un ejemplo del dataset\n",
    "dataset[\"train\"][100]\n",
    "from transformers import AutoTokenizer\n",
    "# Descargar el tokenizador de BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# Función para tokenizar los ejemplos, se aplica truncation y padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "# Aplicar la función a todo el dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Utilizar un subconjunto del dataset reducido si no queremos entrenar con todo el dataset\n",
    "# Si lo utilizamos todo, se tarda mucho en entrenar\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con Pytorch Trainer\n",
    "\n",
    "La librería Transformers proporciona una clase *Trainer* optimizada para el entrenamiento de modelos Transformer. Evita que tengamos que escribir nuestro propio bucle de entrenamiento. El API *Trainer* permite la configuración de una gama amplia de parámetros de configuración del entrenamiento y de características adicionales como *logging*, *gradiente accumulation* y *mixed precision*.\n",
    "\n",
    "El primer paso es cargar tu modelo y especificar el número de etiquetas (*labels*) esperadas. En la documentación del dataset de [Yelp](https://huggingface.co/datasets/yelp_review_full#data-fields) vemos que hay 5 etiquetas.\n",
    "\n",
    "Por defecto, los pesos se cargan en precisión total (*torch.float32*), sin importar si los datos estaban almacenados en una precisión más baja, por ejemplo *half precision* (*torch.float16*). Si queremos evitar esto, y que el modelo se cargue en la precisión en la que fue almacenado, tenemos que fijar el parámetro *torch_dtype=\"auto\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07be3f164a044d8390539f7643504761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de entrenamiento\n",
    "\n",
    "Para fijar los hiperparámetros ajustables del entrenamiento, crear una clase llamada *TrainingArguments* que contiene todos los hiperparámetros que puede ajustar además de *flags* para activar distintas opciones de entrenamiento. En esta demostración dejaremos los valores por defecto, pero puedes experimentar a cambiar distintos parámetros y ver qué efecto tienen en el entreanmiento. ([documentación TrainingArguments](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.TrainingArguments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "El Trainer no evalúa automáticamente el rendimiento del modelo durante el entrenamiento. Tendrás que pasarle a Trainer una función para calcular y hacer un reporte de las métricas. La biblioteca de Datasets proporciona una función de accuracy simple que puedes cargar con la función load_metric (ver [este tutorial](https://huggingface.co/docs/datasets/metrics) para más información):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/ulc/es/dac/mytx/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9041439410464d07b4ea0894606d44d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define la función compute en metric para calcular el accuracy de tus predicciones. Antes de pasar tus predicciones a compute, necesitas convertir las predicciones a logits (recuerda que todos los modelos de Transformers devuelven logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quieres controlar tus métricas de evaluación durante el fine-tuning, especifica el parámetro eval_strategy en tus argumentos de entrenamiento para que el modelo tenga en cuenta la métrica de evaluación al final de cada época:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Crea un objeto Trainer con tu modelo, argumentos de entrenamiento, datasets de entrenamiento y de prueba, y tu función de evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, aplica fine-tuning a tu modelo llamando train():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/ulc/es/dac/mytx/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.367411</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.034930</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.984887</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.1327731119791666, metrics={'train_runtime': 356.644, 'train_samples_per_second': 8.412, 'train_steps_per_second': 1.051, 'total_flos': 789354427392000.0, 'train_loss': 1.1327731119791666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen dos vías más complicadas para realizar el mismo proceso, pero que a cambio proporcionan más control sobre el mismo.\n",
    "\n",
    "- Fine-tuning con Keras: [Ejemplo](https://huggingface.co/docs/transformers/v4.47.1/es/training#fine-tuning-con-keras)\n",
    "- Fine-tuning con Pytorch Nativo: [Ejemplo](https://huggingface.co/docs/transformers/v4.47.1/es/training#fine-tune-en-pytorch-nativo)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytx",
   "language": "python",
   "name": "mytx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
